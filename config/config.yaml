# Base configuration for LAPA project
# This is the entry point for Hydra configuration composition

defaults:
  - _self_
  - experiment: laq_oxe_local  # Default experiment (override with experiment=X)
  - override hydra/hydra_logging: disabled  # Disable Hydra's own logging
  - override hydra/job_logging: none        # We handle logging ourselves

# Global settings
experiment_name: ${experiment.name}
seed: 42
precision: "bf16-mixed"  # "32-true", "16-mixed", "bf16-mixed"

# Hydra configuration (default; submit_job overrides these to keep snapshots inside the run dir)
hydra:
  run:
    # Keep Hydra's `.hydra/` snapshots inside the same run directory as our unified logging.
    # `logging.runs_dir` is always a string (computed by default) and can be overridden on the CLI.
    dir: ${logging.runs_dir}
  sweep:
    dir: ${oc.select:logging.root_dir,.}/runs/sweeps/${now:%Y-%m-%d_%H-%M-%S}_${experiment.name}
    subdir: ${hydra.job.num}

# Logging configuration
logging:
  level: INFO                # Logging level (DEBUG, INFO, WARNING, ERROR)
  job_id: null               # Optional explicit run ID (used by submit_job local runs)
  # Base directory for all run artifacts (relative paths resolve against the repo CWD).
  # Note: Hydra's `hydra.run.dir` expects a string; leaving this as `null` breaks
  # interpolation in some OmegaConf versions.
  root_dir: .
  # Run directory for this invocation (used by Hydra + unified logging).
  # Override on the CLI to write runs elsewhere.
  runs_dir: ${logging.root_dir}/runs/${now:%Y-%m-%d_%H-%M-%S}_${experiment.name}

  # WandB integration
  use_wandb: true            # Enable Weights & Biases logging
  project: hlrp              # WandB project name
  log_model: false           # Upload model checkpoints to WandB
  tags: []                   # WandB tags for run organization

  # Output structure (under logging.runs_dir):
  #   unified.log      - All logger.info() calls (timestamped)
  #   wandb/           - WandB files (includes print() capture)
  #   checkpoints/     - Model checkpoints
  #   .hydra/          - Hydra config snapshots (when hydra.run.dir is set to logging.runs_dir)

# Paths / caching
paths:
  # Base directory for large caches (HF weights, torch hub, TFDS cache, etc.).
  # If relative, resolved against `logging.root_dir` if set, else project root.
  cache_dir: cache

# Job submission defaults (used by scripts/submit_job.py)
submit:
  # Training entrypoint (without .py). Experiments can override this.
  script: 2_train_laq
  # If true, prints the generated sbatch script(s) but does not submit.
  dry_run: false
  # Where to persist pretrained model downloads (HF + torch/torchvision).
  cache_dir: cache
  # Optional shell commands to run before launching the training command.
  # Typical use case: editable-install a local LeRobot plugin package.
  pre_commands: []

# Lightweight benchmarking knobs (kept for optional local profiling scripts)
benchmark:
  warmup_steps: 20
  steps: 200
  # Optional: sleep after each batch fetch to simulate model compute time.
  compute_sleep_s: 0.0
  tf_profile: false
  torch_profile: false
  # Optional: prime dataset pipelines before benchmarking.
  prime_each_dataset_samples: 0
  prime_materialize: false
  # Python-prefetch tuning knobs.
  python_prefetch_queue_size: 1
  python_prefetch_min_ready_datasets: 4
