# Foundation VLA Model Configuration
# Vision-Language-Action model with 7B LLM backbone

name: vla_7b

vision:
  siglip:
    model_name: google/siglip-so400m-patch14-384
    freeze: true
    output_dim: 1152
  dinov2:
    model_name: facebook/dinov2-large
    freeze: true
    output_dim: 1024

projector:
  input_dim: 2176  # 1152 + 1024
  hidden_dim: 4096
  output_dim: 4096  # Match Llama embedding dim
  num_layers: 2
  activation: gelu
  dropout: 0.0

llm:
  model_name: meta-llama/Llama-2-7b-hf
  freeze_embeddings: false
  freeze_layers: []  # Empty = train all layers
  use_flash_attention: true
  gradient_checkpointing: false  # Enable for 13B+

action_head:
  type: latent  # 'latent' for Stage 2, 'continuous' for Stage 3
  input_dim: 4096
  
  # Latent action prediction (Stage 2)
  latent:
    vocab_size: 8
    num_tokens: 4
  
  # Continuous action prediction (Stage 3)
  continuous:
    num_actions: 7  # [x, y, z, roll, pitch, yaw, gripper]
    num_bins: 256
    dropout: 0.1

