# LAQ Model Configuration
# ViT-based VQ-VAE for latent action quantization (LAPA architecture)

name: laq_vit

# Model architecture (matches LAPA's LatentActionQuantization)
dim: 1024                # Transformer embedding dimension
quant_dim: 32           # Quantization dimension
codebook_size: 8        # Number of codebook entries
code_seq_len: 4         # Length of quantized action sequence
vq_discarding_threshold: 0.1  # Replacement cutoff as fraction of average usage (see NSVQ.replace_unused_codebooks)
image_size: 256         # Input image size
patch_size: 32          # Patch size for ViT
spatial_depth: 8        # Number of spatial transformer layers
temporal_depth: 8       # Number of temporal transformer layers
dim_head: 64            # Attention head dimension
heads: 16               # Number of attention heads
channels: 3             # RGB channels
attn_dropout: 0.0       # Attention dropout
ff_dropout: 0.0         # Feed-forward dropout

use_dinov3_encoder: true
dinov3_model_name: "facebook/dinov3-vits16-pretrain-lvd1689m"
dinov3_pool_to_grid: 8

# Latent ablation (debug)
# - none: normal training
# - permute_batch: randomly permute action tokens across batch to break alignment
latent_ablation: none

# Training decoder flags (at least one must be enabled, or flow must be configured)
use_dino_decoder: true  # Predict next frame's encoder tokens (DINO or pixel-projected)
use_pixel_decoder: false # Predict next frame's pixels with gradients to encoder

flow:
  enabled: true
  model: raft_small    # "raft_small" or "raft_large"
  loss_weight: 0.1     # Weight for flow loss in total loss
  decoder_depth: 2     # Transformer depth for flow decoder
  warmup_steps: 10000
  teacher_num_flow_updates: 6
  teacher_chunk_size: 64

# Interpretability decoder (optional, for visualization only)
use_aux_decoder: true   # Predict pixels for visualization (gradients detached from encoder)

# Codebook replacement schedule
# Replace unused codebook entries at diminishing frequency to improve utilization
# Format: list of [interval, until_step] pairs
# - interval: replace every N steps
# - until_step: stop this interval at this step
# Example: [[10, 100], [100, 1000]] means:
#   - Replace every 10 steps for first 100 steps
#   - Replace every 100 steps for steps 100-1000
codebook_replace_schedule:
  - [10, 100]      # Every 10 steps for first 100 steps
  - [100, 1000]    # Every 100 steps for steps 100-1000
  - [500, 5000]    # Every 500 steps for steps 1000-5000
  - [1000, 10000]  # Every 1000 steps for steps 10000-100000
  - [5000, 50000]  # Every 5000 steps for steps 100000-500000
  - [10000, 100000] # Every 10000 steps for steps 100000-1000000

# Optional: Flow supervision config
# Enriches latent space with motion information via RAFT knowledge distillation
# To enable, set flow config in experiment (see laq_oxe_all_val_3.yaml)
# flow:
#   enabled: true        # Set false to disable flow without removing the block
#   model: raft_small    # "raft_small" or "raft_large"
#   loss_weight: 0.1     # Weight for flow loss in total loss
#   decoder_depth: 4     # Transformer depth for flow decoder
#   warmup_steps: 10000
#   # Performance knobs (optional)
#   teacher_num_flow_updates: 12  # RAFT refinement iterations (smaller = faster)
#   teacher_chunk_size: 64        # Larger = faster, more GPU memory
