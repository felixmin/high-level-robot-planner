# LAPA Model Configuration
# Transformer-based architecture for latent action quantization

name: laq_transformer  # Changed from laq_base

# Image and patch configuration
image_size: 256  # Changed from 224 for better patch alignment
patch_size: 32  # 256/32 = 8×8 = 64 patches
channels: 3

# Transformer dimensions
dim: 1024  # Model dimension
quant_dim: 32  # Quantization dimension

# Encoder configuration
spatial_depth: 8  # Spatial transformer layers
temporal_depth: 8  # Temporal transformer layers
heads: 16  # Attention heads
dim_head: 64  # Dimension per head (heads × dim_head = dim)
mlp_ratio: 4  # FFN expansion ratio

# NSVQ configuration (replaces old quantizer)
code_seq_len: 4  # 2×2 action grid = 4 tokens
codebook_size: 8  # Single shared codebook
# NO beta, NO ema_decay - LAPA uses noise-substitution STE

# Decoder configuration
decoder_depth: 8  # Cross-attention decoder layers
decoder_heads: 16  # Decoder attention heads (usually same as encoder)

# Dropout (typically 0.0 for deterministic training)
dropout: 0.0

# Training parameters (can be overridden by training config)
learning_rate: 1e-4
weight_decay: 0.01
warmup_steps: 1000

# NO loss_weights section - LAPA uses ONLY MSE loss
# Removed: reconstruction, codebook, commitment weights

# Notes:
# - LAPA uses MSE loss only (no VQ-specific losses)
# - Input format: [B, 3, 2, 256, 256]
# - Output format: [B, 3, 1, 256, 256]
# - Single shared codebook (not per-position)
# - Delta quantization (last_frame - first_frame)
