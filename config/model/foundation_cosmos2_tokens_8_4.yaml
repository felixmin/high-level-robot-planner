# Stage 2 (Foundation) token-based VLA using Cosmos-Reason2 (Qwen3-VL).
#
# This variant is for LAQ checkpoints with codebook_size=8 and code_seq_len=4
# (e.g. LAQ experiment `laq_oxe_all_val_3`).

name: vla_cosmos2_tokens

backend: qwen3vl_chat_tokens

vla:
  model_name: nvidia/Cosmos-Reason2-2B
  torch_dtype: bf16  # bf16, fp16, fp32
  attn_implementation: sdpa

laq:
  checkpoint: ""  # required at runtime (Stage 1 LAQ checkpoint)

action_tokens:
  codebook_size: 8
  code_seq_len: 4
  action_start: "<ACTION>"
  action_end: "</ACTION>"
  token_fmt: "<ACT_{i}>"

chat:
  system_prompt: "You are a robot policy. Reply only with action tokens."
