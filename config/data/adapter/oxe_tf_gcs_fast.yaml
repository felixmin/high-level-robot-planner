# @package data.adapter

# TF OXE adapter configuration optimized for *GCS streaming* (local training).
#
# Motivation:
# - With many datasets and `mix_block_length=1`, `sample_from_datasets()` can incur
#   large stalls when it switches to a dataset whose pipeline has no buffered
#   elements yet (especially right after startup).
# - A small per-dataset prefetch + block sampling reduces cross-dataset switching
#   and hides some of the remote I/O latency.
#
# This is intentionally conservative (small buffers) to avoid blowing up RAM.

defaults:
  - oxe_tf_low_ram
  - _self_

tf:
  # Reduce shuffle buffers vs `oxe_tf_low_ram` to avoid large "startup" reads from GCS.
  # (Shuffle is still enabled; this is a throughput/latency trade-off.)
  train:
    # Buffering *episodes* is expensive on GCS (episodes can be large), and tf.data
    # shuffle fills the buffer before yielding the first element. Keep these off for
    # predictable startup latency when training directly from `gs://...`.
    episode_queue_shuffle_buffer: 0
    intra_episode_sample_shuffle_buffer: 0
    # Global (post-mix) shuffle is expensive for GCS streaming because it buffers *fully
    # decoded samples* (incl. images). Keep it off and rely on per-episode + dataset
    # mixing for randomness.
    global_stream_shuffle_buffer: 0

  prefetch:
    # Keep a small buffer per dataset so the mixer is less likely to block when it switches.
    # Note: with `mix_block_length>1`, this buffers *blocks* (not individual samples).
    per_dataset_stream_buffer: 1
    # Keep some post-mix buffering (batched).
    final_stream_buffer: 8

  mixing:
    # Sample blocks to amortize dataset switching overhead.
    # For large-batch training, mixing at (or above) batch granularity avoids
    # per-sample cross-dataset switching stalls.
    mix_block_length: 128
    # Repeat each dataset choice for N consecutive batches (keeps memory constant
    # while reducing cross-dataset switching overhead on GCS).
    selector_run_length: 1
    # `choose_from_datasets` tends to be more predictable than weighted sampling in some settings.
    strategy: choose
    # Avoid spawning N_datasets * M_threads private pools (can degrade CPU throughput).
    per_dataset_private_threadpool_size: 0
    # IMPORTANT: scale per-dataset parallelism down as dataset count grows
    # (otherwise AUTOTUNE/threadpools multiply by N_datasets and can OOM).
    parallelism_mode: sqrt

  # Allow more concurrency on GCS to hide network latency.
  tfds_read:
    cycle_length: 8
    decode_parallelism: 4
    interleave_parallelism: 4

  pipeline:
    episode_concurrency: 4
    transform_parallelism: 8
    interleave_parallelism: 4
