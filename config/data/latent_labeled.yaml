# Latent-Labeled Dataset Configuration
# Generated by LAQ inference, used for foundation training

name: openx_latent_labeled
task: foundation

# Data paths (output from script 3_generate_latent_labels.py)
train_shards: /dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/datasets/openx_latent_labeled/train_shard_{00000..00099}.tar
val_shards: /dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/datasets/openx_latent_labeled/val_shard_{00000..00009}.tar

# DataLoader settings
batch_size: 16  # Smaller due to 7B model memory
num_workers: 8
prefetch_factor: 4
pin_memory: true
persistent_workers: true

shuffle_buffer: 1000
resample_shards: true

# Image preprocessing
image_size: 224
normalize:
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

# Text settings
max_text_length: 77  # Similar to CLIP
tokenizer: meta-llama/Llama-2-7b-hf

# Minimal augmentation (model should generalize)
augmentation:
  random_horizontal_flip: 0.0
  color_jitter:
    brightness: 0.0
    contrast: 0.0
    saturation: 0.0
    hue: 0.0
  random_erasing: 0.0

