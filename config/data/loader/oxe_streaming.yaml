# @package data.loader

# PyTorch DataLoader settings for streaming (IterableDataset) backends.
# Note: shuffle happens inside the adapter (tf.data / HF streaming), not in DataLoader.

batch_size: 32
num_workers: 0  # should be 0 for streamed dataset as otherwise
                # it adds IPC overhead and sync issues...
                # parallelism is handled by tfds directly
                # also dlpack apparently only works in the same process
pin_memory: true
prefetch_factor: null

