# @package _global_

# Minimal local OXE run (RAM-safe):
# - no shuffle buffers
# - no prefetching
# - no tf.data AUTOTUNE
# - tiny splits + tiny batch size

defaults:
  - /model@model: laq
  - /data@data: laq_oxe_all
  - /training@training: laq_optimizer
  - /cluster@cluster: local_dev

experiment:
  name: laq_oxe_local
  description: "Minimal local OXE run (no shuffle/prefetch/autotune)"

model:
  flow:
    model: raft_small    # "raft_small" or "raft_large"
    loss_weight: 0.1     # Weight for flow loss in total loss
    decoder_depth: 8     # Transformer depth for flow decoder
    warmup_steps: 10000

data:
  batch_size: 32
  num_workers: 0

  # tf.data settings (tune in experiments)
  episode_shuffle_buffer: 0
  pair_shuffle_buffer: 0
  val_episode_shuffle_buffer: 0
  val_pair_shuffle_buffer: 0
  episode_prefetch_buffer: 0
  prefetch_buffer: 32
  num_parallel_episodes: 1
  num_parallel_calls: 1
  # Multi-dataset only: sample blocks per-dataset in tf.data to reduce
  # `sample_from_datasets()` coordination overhead (1 = per-sample mixing).
  multi_dataset_mix_block_length: 1
  # Multi-dataset only: how to allocate `num_parallel_*` across datasets.
  # - divide: treat `num_parallel_*` as total budget and split across datasets (default)
  # - sqrt: split by sqrt(n_datasets) (middle ground)
  # - full: pass full `num_parallel_*` to each dataset (can oversubscribe)
  multi_dataset_parallelism_mode: divide
  # Multi-dataset only: optional small per-dataset prefetch to reduce
  # `sample_from_datasets()` switching stalls (0 disables).
  multi_dataset_per_dataset_prefetch_buffer: 0
  # Multi-dataset only: mixing implementation.
  # - sample: tf.data `sample_from_datasets()` (default)
  # - choose: `choose_from_datasets()` with a shuffled selector (approx weighted)
  multi_dataset_mixing_strategy: sample
  # Multi-dataset only: private thread pool size for each dataset pipeline.
  # 0 = use shared global threadpool (recommended for batch_size>=256)
  # >0 = each pipeline gets its own threadpool (helps for batch_size<=128)
  # - batch_size=128: private_threadpool=64 gives +20% throughput
  # - batch_size=256: shared threadpool (0) is faster
  multi_dataset_private_threadpool_size: 64

  # Reduce output/overhead
  return_metadata: false
  persistent_iterator: true
  samples_per_episode: 0
  sampling_seed: 0

training:
  enable_progress_bar: true
  enable_model_summary: true
  scheduler:
    type: none
  profiler:
    enabled: true
    type: advanced
    filename: fit-profile
  throughput:
    enabled: true
  dataset_usage_logger:
    enabled: false
  validation:
    check_interval: 1000
    limit_batches: 500
    max_cached_samples: 0
    num_fixed_samples: 0
    num_random_samples: 0

logging:
  level: INFO
  use_wandb: true
  project: hlrp