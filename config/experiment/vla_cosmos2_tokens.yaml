# @package _global_

# Stage 2: Cosmos-Reason2 (Qwen3-VL) token-based VLA (non-debug).
# Uses LAQ code settings with codebook_size=8 and code_seq_len=4.

defaults:
  - /model@model: foundation_cosmos2_tokens_8_4
  - /data@data: oxe_local_indexed
  - /training@training: vla_lightning
  - /cluster@cluster: mcml_h100
  - optional /user_config: local

experiment:
  name: vla_cosmos2_tokens
  description: "Cosmos-Reason2 token-based latent action prediction (8x4 codes)"

submit:
  script: 4_train_foundation

data:
  loader:
    batch_size: 64

logging:
  tags: ["production", "foundation", "cosmos2", "tokens"]
  log_model: true
