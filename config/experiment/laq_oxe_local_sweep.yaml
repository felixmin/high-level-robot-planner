# @package _global_

# Minimal local OXE run (RAM-safe):
# - no shuffle buffers
# - no prefetching
# - no tf.data AUTOTUNE
# - tiny splits + tiny batch size

defaults:
  - /model@model: laq
  - /data@data: laq_oxe_all
  - /training@training: laq_optimizer
  - /cluster@cluster: local_dev

hydra:
  mode: MULTIRUN
  sweeper:
    params:
      #data.episode_queue_shuffle_buffer: 0
      #data.intra_episode_sample_shuffle_buffer: 0
      #data.global_stream_shuffle_buffer: 0
      data.episode_queue_prefetch_buffer: 0
      data.per_dataset_stream_prefetch_buffer: 0
      data.final_stream_prefetch_buffer: 0
      data.num_parallel_episodes: 1
      data.num_parallel_calls: 0,1,2,4,8
      data.per_dataset_private_threadpool_size: 64
      data.multi_dataset_mix_block_length: 1



experiment:
  name: laq_oxe_local_sweep_num_par_calls_${data.num_parallel_calls}
  description: "Minimal local OXE run (no shuffle/prefetch/autotune)"

model:
  flow:
    
    model: raft_large    # "raft_small" or "raft_large"
    loss_weight: 0.1     # Weight for flow loss in total loss
    decoder_depth: 8     # Transformer depth for flow decoder
    warmup_steps: 10000

data:
  batch_size: 32
  num_workers: 0

  # tf.data settings (tune in experiments)
  episode_queue_shuffle_buffer: 0
  intra_episode_sample_shuffle_buffer: 0
  global_stream_shuffle_buffer: 0
  val_episode_queue_shuffle_buffer: 0
  val_intra_episode_sample_shuffle_buffer: 0
  val_global_stream_shuffle_buffer: 0
  episode_queue_prefetch_buffer: 0
  final_stream_prefetch_buffer: 0
  num_parallel_episodes: 1
  num_parallel_calls: 1
  # Multi-dataset only: sample blocks per-dataset in tf.data to reduce
  # `sample_from_datasets()` coordination overhead (1 = per-sample mixing).
  multi_dataset_mix_block_length: 1
  # Multi-dataset only: how to allocate `num_parallel_*` across datasets.
  # - divide: treat `num_parallel_*` as total budget and split across datasets (default)
  # - sqrt: split by sqrt(n_datasets) (middle ground)
  # - full: pass full `num_parallel_*` to each dataset (can oversubscribe)
  multi_dataset_parallelism_mode: divide
  # Multi-dataset only: optional small per-dataset prefetch to reduce
  # `sample_from_datasets()` switching stalls (0 disables).
  per_dataset_stream_prefetch_buffer: 0
  # Multi-dataset only: mixing implementation.
  # - sample: tf.data `sample_from_datasets()` (default)
  # - choose: `choose_from_datasets()` with a shuffled selector (approx weighted)
  multi_dataset_mixing_strategy: sample
  # Multi-dataset only: private thread pool size for each dataset pipeline.
  # 0 = use shared global threadpool (recommended for batch_size>=256)
  # >0 = each pipeline gets its own threadpool (helps for batch_size<=128)
  # - batch_size=128: private_threadpool=64 gives +20% throughput
  # - batch_size=256: shared threadpool (0) is faster
  per_dataset_private_threadpool_size: 64

  # Reduce output/overhead
  return_metadata: true
  persistent_iterator: true
  samples_per_episode: 0
  sampling_seed: 42

training:
  max_steps: 500
  enable_progress_bar: true
  enable_model_summary: true
  scheduler:
    type: none
  profiler:
    enabled: true
    type: advanced
    filename: fit-profile
  throughput:
    enabled: true
  dataset_usage_logger:
    enabled: false
  validation:
    check_interval: 1000
    # Keep local validations snappy; increase for more reliable metrics.
    limit_batches: 50
    max_cached_samples: 10
    num_fixed_samples: 4
    num_random_samples: 4

    # --- Validation Strategies ---
    # NOTE: these must be nested under `strategies:`. If they are aligned with
    # `strategies:`, Hydra/OmegaConf will set `strategies` to null and the
    # callbacks will register 0 strategies (no reconstructions logged).
    strategies:
      # Reconstructions (wandb images) + per-bucket grids.
      basic:
        enabled: true
        every_n_validations: 1
        num_fixed_samples: 4
        num_random_samples: 4
        visualize_train: true
        visualize_val: true
        visualize_per_bucket: true

      # Flow comparisons (only logs if `model.flow.enabled=true` and flow teacher/decoder exist).
      flow_visualization:
        enabled: true
        every_n_validations: 1
        num_samples: 8

      # Disable code-heavy analyses by default for local runs (they can
      # substantially slow validation by requiring per-batch code extraction).
      latent_transfer:
        enabled: false
      sequence_examples:
        enabled: false
      codebook_histogram:
        enabled: false
      sequence_histogram:
        enabled: false
      all_sequences_histogram:
        enabled: false

logging:
  level: INFO
  use_wandb: true
  project: hlrp
