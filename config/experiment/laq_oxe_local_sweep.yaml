# @package _global_

# Minimal local OXE run (RAM-safe):
# - no shuffle buffers
# - no prefetching
# - no tf.data AUTOTUNE
# - tiny splits + tiny batch size

defaults:
  - /model@model: laq
  - /data@data: laq_oxe_all
  - /training@training: laq_optimizer
  - /cluster@cluster: local_dev

hydra:
  mode: MULTIRUN
  sweeper:
    params:
      data.adapter.tf.prefetch.episode_queue_buffer: 0
      data.adapter.tf.prefetch.per_dataset_stream_buffer: 0
      data.adapter.tf.prefetch.final_stream_buffer: 0
      data.adapter.tf.tfds_read.cycle_length: 1
      data.adapter.tf.pipeline.episode_concurrency: 1
      data.adapter.tf.pipeline.transform_parallelism: 0,1,2,4,8
      data.adapter.tf.pipeline.interleave_parallelism: 0
      data.adapter.tf.mixing.per_dataset_private_threadpool_size: 64
      data.adapter.tf.mixing.mix_block_length: 1



experiment:
  name: laq_oxe_local_sweep_transform_par_${data.adapter.tf.pipeline.transform_parallelism}
  description: "Minimal local OXE run (no shuffle/prefetch/autotune)"

model:
  flow:
    enabled: false
    model: raft_large    # "raft_small" or "raft_large"
    loss_weight: 0.1     # Weight for flow loss in total loss
    decoder_depth: 8     # Transformer depth for flow decoder
    warmup_steps: 10000

data:
  loader:
    batch_size: 32
    num_workers: 0

  preprocess:
    return_metadata: true

  adapter:
    tf:
      train:
        episode_queue_shuffle_buffer: 0
        intra_episode_sample_shuffle_buffer: 0
        global_stream_shuffle_buffer: 0
      val:
        episode_queue_shuffle_buffer: 0
        intra_episode_sample_shuffle_buffer: 0
        global_stream_shuffle_buffer: 0
      tfds_read:
        cycle_length: 1
      pipeline:
        episode_concurrency: 1
        transform_parallelism: 0
        interleave_parallelism: 0
      prefetch:
        episode_queue_buffer: 0
        final_stream_buffer: 0
        per_dataset_stream_buffer: 0
      mixing:
        mix_block_length: 1
        parallelism_mode: divide
        strategy: sample
        per_dataset_private_threadpool_size: 64
      iterator:
        persistent: true
      sampling:
        samples_per_episode: 0
        seed: 42

training:
  max_steps: 500
  progress_bar:
    enabled: true
    refresh_rate: 10
    leave: false
  model_summary:
    enabled: true
  scheduler:
    type: none
  profiler:
    enabled: true
    type: advanced
    filename: fit-profile
  throughput:
    enabled: true
  dataset_usage_logger:
    enabled: false
  validation:
    check_interval: 1000
    # Keep local validations snappy; increase for more reliable metrics.
    limit_batches: 50
    max_cached_samples: 10
    num_fixed_samples: 4
    num_random_samples: 4

    # --- Validation Strategies ---
    # NOTE: these must be nested under `strategies:`. If they are aligned with
    # `strategies:`, Hydra/OmegaConf will set `strategies` to null and the
    # callbacks will register 0 strategies (no reconstructions logged).
    strategies:
      # Reconstructions (wandb images) + per-bucket grids.
      basic:
        enabled: true
        every_n_validations: 1
        num_fixed_samples: 4
        num_random_samples: 4
        visualize_train: true
        visualize_val: true
        visualize_per_bucket: true

      # Flow comparisons (only logs if `model.flow.enabled=true` and flow teacher/decoder exist).
      flow_visualization:
        enabled: true
        every_n_validations: 1
        num_samples: 8

      # Disable code-heavy analyses by default for local runs (they can
      # substantially slow validation by requiring per-batch code extraction).
      latent_transfer:
        enabled: false
      sequence_examples:
        enabled: false
      codebook_histogram:
        enabled: false
      sequence_histogram:
        enabled: false
      all_sequences_histogram:
        enabled: false

logging:
  level: INFO
  use_wandb: true
  project: hlrp
