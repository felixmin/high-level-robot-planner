# @package _global_

# Continue training from 80k flow checkpoint with 32 OXE datasets
# - Loads model weights only (fresh optimizer/scheduler)
# - Uses new 32-dataset mixture with Python keep-hot mixing
# - ~8 hours training run

defaults:
  - /model@model: laq
  - /data@data: laq_oxe_cluster_mirror_large_gcs_fast
  - /training@training: laq_optimizer
  - /cluster@cluster: local_dev
  - optional /user_config: local

experiment:
  name: laq_32dataset_continue
  description: "Continue LAQ from 80k flow checkpoint with 32 OXE datasets"

# Different seed for variety
seed: 250000

# Keep same model settings as the 80k checkpoint
model:
  image_size: 256
  use_dinov3_encoder: true
  dinov3_model_name: "facebook/dinov3-vits16-pretrain-lvd1689m"
  dinov3_pool_to_grid: 8
  code_seq_len: 4
  codebook_size: 8
  use_dino_decoder: true
  use_pixel_decoder: false
  use_aux_decoder: true
  # Disable codebook replacement - model is already well-trained
  codebook_replace_schedule: []

  # Flow supervision
  flow:
    enabled: true
    model: raft_small
    loss_weight: 0.1
    decoder_depth: 2
    warmup_steps: 500  # Short warmup since model already trained
    teacher_num_flow_updates: 6
    teacher_chunk_size: 64

data:
  loader:
    batch_size: 64
    num_workers: 0

training:
  # Load weights from the 80k flow-trained checkpoint
  load_weights_from: "/mnt/data/workspace/runs/hlrp/2026-02-02_10-58-35_laq_continue_with_flow/checkpoints/laq-stepstep=080000.ckpt"

  # ~8 hours at ~2.5 it/s = ~72k steps, round to 75k
  max_steps: 75000
  epochs: 1000

  # Cosine decay for continued training
  scheduler:
    type: cosine
    warmup_steps: 500
    warmup_start_lr: 5.0e-5
    min_lr: 1.0e-6

  validation:
    check_interval: 2500
    limit_batches: 200
    max_cached_samples: 256
    num_fixed_samples: 8
    num_random_samples: 8

    # No bucket filters since return_metadata=false
    buckets: {}

    strategies:
      basic:
        enabled: true
        every_n_validations: 1
        visualize_train: true
        visualize_val: true
      codebook_histogram:
        enabled: true
        every_n_validations: 2
      sequence_histogram:
        enabled: true
        every_n_validations: 2
        num_top_sequences: 30
      flow_visualization:
        enabled: true
        every_n_validations: 2
        num_samples: 16

logging:
  level: INFO
  use_wandb: true
  project: hlrp
  tags: ["32-dataset", "continue", "flow", "laq"]
