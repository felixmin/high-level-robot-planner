# @package _global_

# HuggingFace-based OXE training (no TensorFlow required)
# Uses jxu124/OpenX-Embodiment on HuggingFace Hub

defaults:
  - /model@model: laq
  - /data@data: laq_hf_bridge
  - /training@training: laq_optimizer
  - /cluster@cluster: local_dev

experiment:
  name: laq_hf_local
  description: "LAQ training with HuggingFace OXE backend (no TensorFlow)"

model:
  flow:
    model: raft_small
    loss_weight: 0.1
    decoder_depth: 8
    warmup_steps: 10000

data:
  preprocess:
    return_metadata: false
  loader:
    batch_size: 32
    num_workers: 0  # HF streaming: keep 0 (WebDataset sharding issues)

training:
  enable_progress_bar: true
  enable_model_summary: true
  scheduler:
    type: none
  profiler:
    enabled: false
  throughput:
    enabled: true
  dataset_usage_logger:
    enabled: false
  validation:
    check_interval: 1000
    limit_batches: 100
    max_cached_samples: 0
    num_fixed_samples: 0
    num_random_samples: 0
    buckets: null

logging:
  level: INFO
  use_wandb: true
  project: hlrp
