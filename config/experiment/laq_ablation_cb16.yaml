# @package _global_

# Ablation: Codebook size 16 (vs default 8)
# Tests if larger codebook improves action representation

defaults:
  - /model@model: laq
  - /data@data: laq_oxe_all_low_ram
  - /training@training: laq_optimizer
  - /cluster@cluster: local_dev
  - optional /user_config: local

experiment:
  name: laq_ablation_cb16
  description: "LAQ with codebook size 16"

model:
  image_size: 256
  use_dinov3_encoder: true
  dinov3_model_name: "facebook/dinov3-vits16-pretrain-lvd1689m"
  dinov3_pool_to_grid: 8
  code_seq_len: 4
  codebook_size: 16  # Ablation: 16 instead of 8
  use_dino_decoder: true
  use_pixel_decoder: false
  use_aux_decoder: true
  flow:
    enabled: false

data:
  loader:
    batch_size: 64
    num_workers: 0

training:
  max_steps: 60000  # ~same as original checkpoint
  epochs: 1000

  validation:
    check_interval: 2500
    limit_batches: 200
    max_cached_samples: 256
    num_fixed_samples: 8
    num_random_samples: 8

    buckets:
      language_table:
        filters: {dataset_name: "language_table"}
        max_samples: 64
      bridge:
        filters: {dataset_name: "bridge"}
        max_samples: 64

    strategies:
      basic:
        enabled: true
        every_n_validations: 1
        visualize_train: true
        visualize_val: true
      codebook_histogram:
        enabled: true
        every_n_validations: 2
      sequence_histogram:
        enabled: true
        every_n_validations: 2
        num_top_sequences: 30

logging:
  level: INFO
  use_wandb: true
  project: hlrp
  tags: ["ablation", "codebook-16", "laq"]
