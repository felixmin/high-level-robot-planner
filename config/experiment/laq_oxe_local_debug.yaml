# @package _global_

# Minimal local OXE run (RAM-safe):
# - no shuffle buffers
# - no prefetching
# - no tf.data AUTOTUNE
# - tiny splits + tiny batch size

defaults:
  - /model@model: laq
  - /training@training: laq_optimizer
  - /cluster@cluster: local_dev

experiment:
  name: laq_oxe_local
  description: "Minimal local OXE run (no shuffle/prefetch/autotune)"

model:
  image_size: 256
  patch_size: 16
  dim: 256
  heads: 4
  dim_head: 32
  spatial_depth: 2
  temporal_depth: 2
  quant_dim: 16

data:
  # Small multi-dataset slice (streams from GCS, limited episodes)
  datasets:
    - name: language_table
      train_split: "train[:200]"
      val_split: "train[200:220]"
      offset: 10
      weight: 0.1
      size: 1000000
    - name: bridge
      train_split: "train[:200]"
      val_split: "train[200:220]"
      offset: 5
      size: 250000
    - name: rt1
      train_split: "train[:200]"
      val_split: "train[200:220]"
      offset: 3
      size: 250000
    - name: robonet
      train_split: "train[:200]"
      val_split: "train[200:220]"
      offset: 10
      size: 250000

  offset: 5
  image_size: 256
  batch_size: 24
  num_workers: 0

  # tf.data settings (tune in experiments)
  episode_queue_shuffle_buffer: 0
  intra_episode_sample_shuffle_buffer: 0
  global_stream_shuffle_buffer: 0
  val_episode_queue_shuffle_buffer: 0
  val_intra_episode_sample_shuffle_buffer: 0
  val_global_stream_shuffle_buffer: 0
  episode_queue_prefetch_buffer: 0
  final_stream_prefetch_buffer: 64
  num_parallel_episodes: 32
  num_parallel_calls: 32
  # Multi-dataset only: sample blocks per-dataset in tf.data to reduce
  # `sample_from_datasets()` coordination overhead (1 = per-sample mixing).
  multi_dataset_mix_block_length: 1
  # Multi-dataset only: how to allocate `num_parallel_*` across datasets.
  # - divide: treat `num_parallel_*` as total budget and split across datasets (default)
  # - sqrt: split by sqrt(n_datasets) (middle ground)
  # - full: pass full `num_parallel_*` to each dataset (can oversubscribe)
  multi_dataset_parallelism_mode: divide
  # Multi-dataset only: optional small per-dataset prefetch to reduce
  # `sample_from_datasets()` switching stalls (0 disables).
  per_dataset_stream_prefetch_buffer: 0
  # Multi-dataset only: mixing implementation.
  # - sample: tf.data `sample_from_datasets()` (default)
  # - choose: `choose_from_datasets()` with a shuffled selector (approx weighted)
  multi_dataset_mixing_strategy: sample
  # Multi-dataset only: private thread pool size for each dataset pipeline.
  # 0 = use shared global threadpool (recommended for batch_size>=256)
  # >0 = each pipeline gets its own threadpool (helps for batch_size<=128)
  # - batch_size=128: private_threadpool=64 gives +20% throughput
  # - batch_size=256: shared threadpool (0) is faster
  per_dataset_private_threadpool_size: 0

  # Reduce output/overhead
  return_metadata: false
  persistent_iterator: true
  samples_per_episode: 0
  sampling_seed: 0

training:
  epochs: 1
  max_steps: 5
  enable_progress_bar: false
  enable_model_summary: false
  scheduler:
    type: none
  profiler:
    enabled: true
    type: simple
    filename: fit-profile
  throughput:
    enabled: false
  dataset_usage_logger:
    enabled: false
  validation:
    check_interval: 1
    limit_batches: 0.0
    max_cached_samples: 0
    num_fixed_samples: 0
    num_random_samples: 0
    val_buckets: null
    strategies: null

logging:
  level: INFO
  use_wandb: false
