# LAQ Training Configuration
# Optimizer and scheduler settings for LAQ training

epochs: 100
max_steps: null  # If set, overrides epochs

# --- Logging / UI ---
# How often Lightning emits scalar logs (to wandb + any configured logger).
log_every_n_steps: 10

# Terminal progress bar (tqdm).
# NOTE: This is separate from `progress_logger` (which prints periodic lines for log files).
progress_bar:
  enabled: true
  refresh_rate: 10
  leave: false

# Lightning model summary (printed at startup).
model_summary:
  enabled: true

# Periodic text logging (useful in non-interactive logs, e.g. SLURM).
progress_logger:
  enabled: true
  log_every_n_steps: 100

# Model metrics: control frequency of expensive computations and logging.
metrics:
  # Log model-provided metrics dict every N steps.
  log_every_n_steps: 10
  # `torch.unique()` over code indices is expensive; compute it only every N steps.
  num_unique_codes_every_n_steps: 50

optimizer:
  type: AdamW
  lr: 1.0e-4
  betas: [0.9, 0.999]
  weight_decay: 0.01
  eps: 1.0e-8

scheduler:
  type: cosine  # 'cosine', 'step', 'plateau', or 'none' to disable
  warmup_steps: 1000
  warmup_start_lr: 1.0e-6
  min_lr: 1.0e-6
  
  # For cosine scheduling: the LR schedule is step-based.
  # Prefer setting `training.max_steps` for streaming / variable-length epochs.
  
  # For StepLR (if used)
  step_size: 30
  gamma: 0.1
  
  # For ReduceLROnPlateau (if used)
  mode: min
  factor: 0.5
  patience: 10

gradient:
  clip_val: 1.0
  clip_algorithm: norm  # 'norm' or 'value'

# Loss configuration
loss_weights:
  reconstruction: 1.0
  vq: 1.0  # Combined codebook + commitment

# Validation settings
validation:
  # How often to run validation (can be float for percentage of epoch, or int for steps)
  check_interval: 500

  # Limit validation batches to save time (0.1 = 10% of val set, or int for exact batches)
  # Set to 1.0 to run full validation.
  limit_batches: 1.0

  # Limit validation batches to save time (0.1 = 10% of val set, or int for exact batches)
  # With 6300 val frame pairs, 0.1 = ~630 pairs = ~160 batches at batch_size 4
  # limit_batches: 1.0

  # Fixed pairs: diverse across datasets, tracked every validation
  num_fixed_samples: 8

  # Random pairs: different each time for diversity
  num_random_samples: 8

  # Maximum frame pairs to cache in RAM (prevents OOM on large val sets)
  max_cached_samples: 1024

  # --- Buckets (for per-bucket visualization and analysis) ---
  # NOTE: Buckets belong to validation (not data). They drive:
  # - sample routing into per-bucket caches
  # - per-bucket visualization grids (via filters)
  # Supports operators: ["!=", value], [">", value], ["not_null", true]
  buckets:
    youtube:
      filters:
        dataset_name: "youtube"
      max_samples: 256
    bridge:
      filters:
        dataset_name: "bridge"
      max_samples: 256

  # --- Validation Strategies ---
  strategies:
    # Basic validation (always runs): loss + visualizations
    basic:
      enabled: true
      visualize_train: true
      visualize_val: true
      visualize_per_bucket: true  # Separate grids for each bucket
      samples_per_bucket: 4
      num_train_samples: 8  # Training samples to visualize

    # Latent transfer analysis: test if latent actions transfer between scenes
    latent_transfer:
      enabled: true
      every_n_validations: 2  # Run every 10th validation
      num_pairs: 256  # Number of pairs to test transfer on

    # Sequence examples: visualize frame pairs grouped by exact code sequence
    sequence_examples:
      enabled: true
      every_n_validations: 3
      top_k_sequences: 16  # Number of most frequent sequences to visualize
      examples_per_sequence: 4  # Frame pairs to show per sequence

# Dataset usage logging: prints how much of each dataset was *actually consumed*
# between validations (uses `batch.dataset_name`).
dataset_usage_logger:
  enabled: true
  log_on_validation_end: true
  log_every_n_steps: 0  # 0 = disabled (validation-end summary only)
  key: dataset_name
  top_k: 12

# Throughput logging: logs perf/steps_per_sec and perf/samples_per_sec to wandb
throughput:
  enabled: true
  log_every_n_steps: 10

# Checkpointing
checkpoint:
  monitor: val/loss
  mode: min
  save_top_k: 3
  save_last: true
  every_n_train_steps: 1000

# Profiling
profiler:
  enabled: true           # Enable Lightning SimpleProfiler
  type: simple             # 'simple', 'advanced', or 'pytorch'
  dirpath: ./profiles      # Where to save profile outputs
  filename: profile        # Profile filename
