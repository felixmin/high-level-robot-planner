# Stage 2 (Foundation) Lightning training settings (non-debug defaults).

max_steps: 200000
max_epochs: 1
accumulate_grad_batches: 1
precision: bf16-mixed
gradient_clip_val: 1.0
log_every_n_steps: 10
resume_from_checkpoint: null

optimizer:
  lr: 1.0e-5
  weight_decay: 0.01

validation:
  # Validate every N training steps (epochs are huge with streaming datasets).
  check_interval: 1000
  # Keep validation bounded (int batches or float fraction).
  limit_batches: 50
  check_val_every_n_epoch: 1
  visualization:
    enabled: true
    num_samples: 4
    every_n_val: 1
    include_freeform_pred: false
    freeform_max_new_tokens: 32

train_visualization:
  enabled: true
  num_samples: 4
  every_n_steps: 1000
  include_freeform_pred: false
  freeform_max_new_tokens: 32

throughput:
  enabled: true
  log_every_n_steps: 10

dataset_usage_logger:
  enabled: true
  log_on_validation_end: true
  key: dataset_name
  top_k: 12

progress_logger:
  enabled: true
  log_every_n_steps: 100

checkpoint:
  monitor: val/loss
  mode: min
  save_top_k: 2
  save_last: true
  save_weights_only: false
  every_n_train_steps: 1000

# Optional: log teacher-forced code-token diagnostics on *training* batches.
# Set small (e.g. 1 or 10) for overfit/debug runs; keep null for production.
train_teacher_forced_metrics_every_n_steps: null

# Optional Lightning debug controls.
# - Set `overfit_batches=1` to sanity-check that loss and accuracy improve on a single batch.
# - For TF streaming backends, also set `data.adapter.tf.iterator.cache_first_train_batch=true`
#   to guarantee the *exact* same batch is replayed each step.
# - Use `limit_train_batches` to bound epoch length for non-step-based runs.
overfit_batches: null
limit_train_batches: null

profiler:
  enabled: true
  type: advanced
  dirpath: ./profiles
  filename: profile
