# Foundation VLA Training Configuration
# FSDP-based training for 7B model

epochs: 10
max_steps: null

# Batch size management
effective_batch_size: 4096  # Target across all GPUs
batch_size_per_gpu: 16
gradient_accumulation_steps: 64  # 16 × 64 × 4 GPUs = 4096

optimizer:
  type: AdamW
  lr: 1.0e-5  # Conservative for LLM finetuning
  betas: [0.9, 0.95]  # Adjusted for LLM training
  weight_decay: 0.1
  eps: 1.0e-8

scheduler:
  type: cosine
  warmup_steps: 2000
  warmup_start_lr: 1.0e-7
  min_lr: 1.0e-6

gradient:
  clip_val: 1.0
  clip_algorithm: norm

# FSDP configuration
fsdp:
  sharding_strategy: FULL_SHARD  # FULL_SHARD, SHARD_GRAD_OP, NO_SHARD
  backward_prefetch: true
  forward_prefetch: false
  activation_checkpointing: false  # Enable for 13B+ models
  cpu_offload: false  # H100 has enough memory
  mixed_precision: bf16  # bf16 for H100

# Validation settings
validation:
  interval_steps: 1000  # Validate every N steps
  num_samples_to_log: 16

# Checkpointing
checkpoint:
  save_interval_steps: 1000
  keep_last: 5
  save_best: true
  monitor: val/accuracy
  mode: max
  
  # Distributed checkpoint for FSDP
  use_distributed_checkpoint: true

