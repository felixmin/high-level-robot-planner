# Stage 2 debug training settings (single GPU, quick smoke runs)

max_steps: 20
max_epochs: 1
accumulate_grad_batches: 1
precision: bf16-mixed
gradient_clip_val: 1.0

optimizer:
  lr: 1.0e-5
  weight_decay: 0.01

validation:
  # Validate every N training steps (Stage 1-style) since epochs can be huge.
  check_interval: 100
  # Keep validation fast for smoke runs (int or float in [0,1]).
  limit_batches: 4
  # Epoch-based fallback (usually irrelevant with check_interval set).
  check_val_every_n_epoch: 1
  # Save qualitative samples alongside metrics.
  visualization:
    enabled: true
    num_samples: 4
    every_n_val: 1
    include_freeform_pred: false
    freeform_max_new_tokens: 32

# Save a few training samples (image + instruction + GT/pred tokens).
train_visualization:
  enabled: true
  num_samples: 4
  every_n_steps: 500
  include_freeform_pred: false
  freeform_max_new_tokens: 32

# Log throughput metrics (useful for WandB).
throughput:
  enabled: true
  log_every_n_steps: 10

# Print dataset mix seen between validations (helps sanity-check multi-dataset training).
dataset_usage_logger:
  enabled: true
  log_on_validation_end: true
  key: dataset_name
  top_k: 12

checkpoint:
  monitor: val/loss
  mode: min
  save_top_k: 1
  save_last: true
  save_weights_only: false
  every_n_train_steps: 100

# Optional Lightning debug controls.
overfit_batches: null
limit_train_batches: null

profiler:
  enabled: true
  type: advanced
  dirpath: ./profiles
  filename: profile
