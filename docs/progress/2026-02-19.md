# Progress Update (2026-02-19)

## Goal
- Continue local stage-2 validation/OOM ladder.
- Identify largest stable config, then launch 9h overnight run.

## What was attempted

### 1) Re-run local bs=8 baseline ladder command
- Command family: `experiment=vla_smol_flow_shared`, `data=oxe_local_spe0`, `batch_size=8`, val every 200 (`limit_batches=1`).
- Result: failed before reliable ladder due environment path/network constraints (see below).

### 2) Writable run path workaround
- Tried running local jobs with:
  - `logging.runs_dir=/tmp/hlrp_runs/<exp>`
  - `paths.cache_dir=/tmp/hlrp_cache`
- This fixed run-dir permission errors from `/mnt/data/workspace/runs/...`.

### 3) Cache wiring for offline model init
- Added symlinks:
  - `/tmp/hlrp_cache/huggingface/hub/models--facebook--dinov3-vits16-pretrain-lvd1689m` -> existing local HF cache
  - `/tmp/hlrp_cache/torch/hub/checkpoints/raft_small_C_T_V2-01064c6d.pth` -> existing local torch cache
- Purpose: avoid network fetch stalls during LAQ model init.

### 4) Offline + local-data fallbacks
- Tried strict offline env:
  - `HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1`
- Tried replacing OXE source with local mirror for `language_table`.

## Hard blockers observed

1. **Runs dir permissions inside this execution environment**
- Default local user config points to `/mnt/data/workspace/runs/hlrp/...`.
- In this sandbox, that path is not writable from the running process.
- Workaround used: `logging.runs_dir=/tmp/hlrp_runs/<exp>`.

2. **No GCS access in this execution environment**
- OXE GCS streaming fails with background prefetch errors (`FailedPreconditionError`), so 29-dataset GCS runs cannot start.

3. **No cluster SSH/DNS in this execution environment**
- `ssh ai` fails with DNS resolution error for `login.ai.lrz.de`.

4. **Local language_table mirror is incomplete for TFDS split expected by config**
- Local path has only a subset of shards (`...00000...` present; missing subsequent files).
- Training fails with TFDS `NotFoundError` for missing `language_table-train.tfrecord-00001-of-01024`.

## Added local fallback config files
- `config/data/dataset/oxe_language_table.yaml`
- `config/data/oxe_local_language_table.yaml`

These were added to test fully-local fallback execution (single dataset, local TFDS source).

## Current status
- No valid end-to-end local stage-2 real-data run could be launched from this environment due the blockers above.
- Therefore the bs ladder and 9h overnight launch were **not** completed from this environment.

## Next action once running in normal workstation shell (outside this sandbox)
1. Use normal writable run root (`/mnt/data/workspace/runs/hlrp/...`).
2. Use normal GCS-capable network/auth environment.
3. Re-run ladder:
   - `bs=8 -> 12 -> 16 -> 20 -> 24 -> 32`
   - keep val at `check_interval=200`, `limit_batches=1`, viz off.
   - stop at first unstable/OOM and pick previous bs.
4. Launch 9h overnight in `tmux` with selected bs.

## Update: Local real-data ladder completed (same date)

The runs below were executed successfully on the workstation against `data=oxe_local_spe0` with online LAQ labeling enabled:
- `model.laq.checkpoint=/mnt/data/workspace/code/high-level-robot-planner/laq-stepstep052500.ckpt`
- one-val recipe: `training.max_steps=35`, `training.validation.check_interval=30`, `training.validation.limit_batches=1`
- low-memory adapter overrides:
  - `data.adapter.tf.mixing.strategy=python`
  - `data.adapter.tf.mixing.mix_block_length=4`
  - `data.adapter.tf.mixing.python_prefetch_queue_size=1`
  - `data.adapter.tf.mixing.python_prefetch_min_ready_datasets=1`
  - `data.adapter.tf.prefetch.final_stream_buffer=2`
  - `data.adapter.tf.prefetch.per_dataset_stream_buffer=0`
  - `data.adapter.tf.prefetch.episode_queue_buffer=0`
  - `data.adapter.tf.pipeline.episode_concurrency=1`
  - `data.adapter.tf.pipeline.transform_parallelism=1`
  - `data.adapter.tf.pipeline.interleave_parallelism=1`
  - `data.adapter.tf.tfds_read.source=auto`
  - `data.adapter.tf.sampling.samples_per_episode=0`

Results (all `Exit status: 0`, validation active, artifact exported):
- `bs=8`: log `/tmp/local_s2_oneval_bs8_ckpt_20260219_091922.log`, max RSS `24,368,720 kB`
- `bs=12`: log `/tmp/local_s2_oneval_bs12_ckpt_20260219_092243.log`, max RSS `25,203,564 kB`
- `bs=16`: log `/tmp/local_s2_oneval_bs16_ckpt_20260219_092617.log`, max RSS `28,358,024 kB`
- `bs=20`: log `/tmp/local_s2_oneval_bs20_ckpt_20260219_093003.log`, max RSS `27,314,768 kB`
- `bs=24`: log `/tmp/local_s2_oneval_bs24_ckpt_20260219_093420.log`, max RSS `30,988,972 kB`
- `bs=32`: log `/tmp/local_s2_oneval_bs32_ckpt_20260219_093809.log`, max RSS `28,865,144 kB`

Stress result (`bs=32`, repeated validation):
- config delta:
  - `training.max_steps=220`
  - `training.validation.check_interval=50`
  - `training.validation.limit_batches=2`
- log `/tmp/local_s2_stress_bs32_ckpt_20260219_094330.log`
- failed with `Exit status: 137` (process killed)
- peak RSS before kill: `38,405,724 kB`

Interpretation:
- short one-val probes are stable through `bs=32`
- repeated validation cycles at `bs=32` are not stable in current setup

## Cluster TODO: source/validation-limit comparison (submitted)

Goal:
- Compare impact of TFDS source resolution (`auto` vs `gcs`).
- Compare impact of validation batch cap (explicit limit vs default/no override).

Shared settings across all 4 runs:
- `training.max_steps=1000`
- `training.validation.check_interval=400`
- `cluster.compute.time_limit=00:45:00`
- `data.loader.batch_size=48`
- `logging.use_wandb=true`

Submitted jobs (results):

| Job ID | Experiment name | Source | Validation limit override | Status | Result notes |
|---|---|---|---|---|---|
| 5487496 | `cluster_s2_diag_resubmit_gcs_lim4_s1000_v400_t45` | `gcs` | `limit_batches=4` | completed | elapsed `00:23:26`, maxRSS `92.8G`, step/s `1.244`, samples/s `59.73`, val loss `1.0590 -> 1.0350` |
| 5487495 | `cluster_s2_match_resubmit_gcs_lim20_s1000_v400_t45` | `gcs` | `limit_batches=20` | completed | elapsed `00:19:34`, maxRSS `76.1G`, step/s `1.456`, samples/s `69.88`, val loss `1.0665 -> 1.0372` |
| 5487498 | `cluster_s2_match_gcs_nolimit_s1000_v400_t45` | `gcs` | none (uses default training config) | completed | elapsed `00:19:17`, maxRSS `74.7G`, step/s `1.415`, samples/s `67.92`, val loss `1.0698 -> 1.0408` |
| 5487497 | `cluster_s2_match_auto_nolimit_s1000_v400_t45` | `auto` | none (uses default training config) | completed | elapsed `00:18:03`, maxRSS `64.3G`, step/s `1.361`, samples/s `65.33`, val loss `1.0812 -> 1.0428` |

Planned pairwise comparisons:
- Source impact (no explicit limit): `5487498` (`gcs`) vs `5487497` (`auto`).
- Validation-limit impact (`gcs`): `5487495` (`limit=20`) vs `5487498` (no override/default).
- Additional short-limit anchor: `5487496` (`limit=4`) to estimate lower-bound validation overhead.

Observed from this batch:
- All 4 runs completed to step `1000`, produced val at steps `400` and `800`, and exported `artifacts/smolvla_shared_stage2_artifact.pt`.
- Source comparison (`gcs` vs `auto`, both no explicit limit): `gcs` was faster (`67.92` vs `65.33` samples/s, about `+4%`) and had slightly lower val loss at step `800` (`1.0408` vs `1.0428`).
- Validation-limit comparison under `gcs` (`limit=20` vs default/no override): explicit `limit=20` was slightly faster (`69.88` vs `67.92` samples/s) with marginally lower val loss at step `800` (`1.0372` vs `1.0408`).
- The `limit=4` diagnostic run is not a strict apples-to-apples comparison (different data preset), but it was the slowest and highest-RSS run in this batch.
