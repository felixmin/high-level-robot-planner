# Progress Update (2026-02-18)

## Stage-2 local param ladder (bs48, 10 steps)

Goal: isolate unstable/slow knobs by changing one parameter at a time from baseline.

Best result in this ladder:
- Run: `/mnt/data/workspace/runs/hlrp/2026-02-18_01-33-00_paramscan06_tfds_interleave2_bs48_s10`
- Override: `data.adapter.tf.tfds_read.interleave_parallelism=2`
- Throughput: `0.03897 steps/s` (`1.87050 samples/s`)

Confirmed bad settings:
- `data.adapter.tf.pipeline.interleave_parallelism=2` alone fails immediately (`num_parallel_calls=2` with `cycle_length=1`).
- `data.adapter.tf.pipeline.episode_concurrency=2` is valid but slower and higher RAM.

Detailed table is in:
- `docs/data/2026-02-17_local_s2_speed_iteration.md`

## Overnight run launched (tmux)

Session:
- `tmux` session name: `s2_overnight_20260218`

Command:
```bash
conda run -n hlrp python scripts/submit_job.py \
  experiment=vla_smol_flow_shared \
  cluster=local_dev \
  experiment.name=local_s2_overnight_bs48_steps25k_oxe_local_spe0_pscan_best \
  model.laq.checkpoint=/mnt/data/workspace/code/high-level-robot-planner/laq-stepstep052500.ckpt \
  data=oxe_local_spe0 \
  data.loader.batch_size=48 \
  training.max_steps=25000 \
  training.max_epochs=-1 \
  training.validation.check_interval=2000 \
  training.validation.visualization.enabled=true \
  training.train_visualization.enabled=true \
  training.profiler.enabled=false \
  data.adapter.tf.tfds_read.interleave_parallelism=2 \
  logging.use_wandb=true
```

Active run dir:
- `/mnt/data/workspace/runs/hlrp/2026-02-18_01-44-51_local_s2_overnight_bs48_steps25k_oxe_local_spe0_pscan_best`

Monitor:
```bash
tail -f /mnt/data/workspace/runs/hlrp/2026-02-18_01-44-51_local_s2_overnight_bs48_steps25k_oxe_local_spe0_pscan_best/unified.log
```

Check tmux:
```bash
tmux ls
tmux attach -t s2_overnight_20260218
```

## Check result (2026-02-18 morning)

Outcome:
- Run did not reach training steps and did not export any artifact/checkpoint.
- Process was OOM-killed by kernel.

Evidence:
- No active process for this run (`4_train_foundation.py` exited).
- Last `unified.log` entries are still TFDS dataset-construction lines (no `Epoch 0` / step metrics).
- Kernel log shows explicit kill:
  - `Out of memory: Killed process 1934864 (python) ... anon-rss:35627112kB`
  - timestamp: `Feb 18 08:14:10`
  - command was this run's stage-2 process in tmux scope.

## Cluster queue additions (forced GCS source)

Submitted extra comparison jobs with forced GCS source:
- `5486342` (`cluster_s2_bs48_match_local_v6_gcs_lrz2`)
- `5486341` (`cluster_s2_bs48_match_local_v6_gcs_mcml2`)

Current status snapshot:
- Running baseline: `5486298` (`cluster_s2_bs48_spe0_highram_v2_lrz`)
- Pending auto-source pair: `5486339`, `5486338`
- Pending forced-GCS pair: `5486342`, `5486341`

## Stage-2 diagnostics/viz patch

Implemented code updates to improve sample debugging and metrics visibility:
- Fixed val sample JSON behavior for `latent_flow`: keep `gt_codes` even when `pred_codes` is absent.
- Added dataset metadata to sample visualizations/JSON:
  - per-sample `dataset_name` in both train/val records
  - dataset shown in panel `meta` text
- Added richer validation stats in `VLATokenBackendLightningModule`:
  - `val/latent_vector_stats/*` and `val/action_stats/*`
  - includes mean/std, norm means, MSE/MAE, cosine mean, and quantized unique-count fractions
- Added validation batch dataset-mix stats:
  - `val/dataset_mix_unique`
  - `val/dataset_mix_top1_frac`
  - `val/dataset_mix_entropy_norm`
  - top dataset fractions as `val/dataset_mix_frac_<dataset>`
  - stdout line: `[Val][BatchMix] ...`
- Added periodic training-batch dataset composition logging:
  - callback prints `[Train][BatchMix] step=... datasetA=n ...`
  - controlled by `training.dataset_usage_logger.log_batch_composition_every_n_steps`

Tests:
- `tests/test_vla_visualization_callback.py`
- `tests/test_vla_train_visualization_callback.py`
- `tests/test_dataset_usage_logger_callback.py`

## Sync + quick smoke submissions (2026-02-18 evening)

- Local commit:
  - `f6f18c2` (`Add stage2 vector+dataset diagnostics and fix sample viz records`)
- Pushed to `origin/main` and pulled on cluster checkout:
  - cluster HEAD: `622e77a` (merge commit containing `f6f18c2`)

Short Stage-2 verification pair submitted (forced `source=gcs`):
- LRZ: `5486353`
  - run dir: `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-18_17-46-58_cluster_s2_bs48_diag20_gcs_lrz`
- MCML: `5486354`
  - run dir: `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-18_17-46-58_cluster_s2_bs48_diag20_gcs_mcml`

Config for both:
- `training.max_steps=20`
- `training.validation.check_interval=20`
- `training.validation.limit_batches=4`
- `training.validation.visualization.enabled=true`
- `training.train_visualization.enabled=true`
- `training.dataset_usage_logger.log_batch_composition_every_n_steps=5`
- `data.adapter.tf.tfds_read.source=gcs`
- `data.loader.batch_size=48`
- `+submit.time=00:15:00`

Note on queued jobs and updated code:
- `submit_job.py` mounts repo path directly and sets container workdir to that repo path (`scripts/submit_job.py`, see sbatch template section).
- Therefore queued jobs started after this pull use the updated cluster checkout (including the new diagnostics patch).

## Local validation OOM deep-dive (2026-02-18 late evening)

- Detailed note:
  - `docs/data/2026-02-18_local_stage2_validation_oom.md`

- Key results:
  - Reproduced local OOM during repeated python-mixer re-inits (`pid 2286800` killed by kernel at `18:28:40`).
  - Observed repeated log pattern:
    - `python-prefetch mixing: starting 29 workers`
    - `initial ready=1/29`
    - repeated cycles with rising RAM.
  - Implemented adapter fix in `packages/common/adapters/oxe.py`:
    - python-prefetch workers now use timeout-based queue put (`Full` retry) so they can exit when `stop` is set.
  - Post-fix behavior improved (less immediate runaway), but restart-loop pressure still exists for local 29-dataset python-mixer runs.

- Cluster status update:
  - `5486298` finished due `TIMEOUT` (not crash), reached validation at steps 2000 and 4000.
  - Pending queue remains:
    - `5486339`, `5486342`, `5486353`, `5486338`, `5486341`, `5486354`.

## TODOs (requested next)

- Stage 1 (LAQ) new cluster run:
  - Start a fresh Stage-1 training run on cluster with:
    - sequence length `1`
    - codebook size `4096`
    - train steps `200000`
  - Use normal dual-queue submission strategy (`lrz_x100` + `mcml_x100`) and cancel the slower starter.

- Stage 2 analysis + optimization:
  - Analyze latest Stage-2 run behavior and tune config so loss keeps decreasing more consistently.
  - Run focused LR experiments (keep other knobs stable per run) and compare learning curves.
  - Decide whether training is meaningful vs random by adding/using explicit convergence diagnostics.
  - Add a reduced-dataset smoke ladder first (small OXE subset), verify stable learning + validation behavior, then scale dataset count back up to full 29.

- Stage 2 “is this really learning?” metric work:
  - Add a metric/check that distinguishes:
    - random-like predicted token/vector distribution
    - meaningful convergence toward GT latent targets
  - Candidate checks to track per validation:
    - GT vs prediction distribution distance (token histogram KL/JS when tokens available)
    - nearest-codebook accuracy / top-k hit rate (for vector outputs)
    - temporal trend of `pred_vs_gt` distance (MSE/MAE/cosine) with confidence intervals
    - collapse indicators (`unique_pred / unique_gt`, entropy, per-position diversity)

## Local command outcomes (worked vs failed)

### Shared command family (Stage-2 local)

```bash
conda run -n hlrp python scripts/submit_job.py \
  experiment=vla_smol_flow_shared \
  cluster=local_dev \
  model.laq.checkpoint=/mnt/data/workspace/code/high-level-robot-planner/laq-stepstep052500.ckpt \
  data=oxe_local_spe0 \
  data.loader.num_workers=0 \
  data.loader.pin_memory=false \
  data.adapter.tf.sampling.samples_per_episode=0 \
  data.adapter.tf.mixing.strategy=python \
  data.adapter.tf.mixing.mix_block_length=4 \
  data.adapter.tf.mixing.python_prefetch_queue_size=1 \
  data.adapter.tf.mixing.python_prefetch_min_ready_datasets=1 \
  data.adapter.tf.tfds_read.skip_steps_decoding=true \
  data.adapter.tf.pipeline.emit_encoded_pairs=true \
  data.adapter.tf.tfds_read.decode_parallelism=4 \
  data.adapter.tf.tfds_read.interleave_parallelism=1 \
  data.adapter.tf.pipeline.episode_concurrency=1 \
  data.adapter.tf.pipeline.transform_parallelism=4 \
  data.adapter.tf.pipeline.interleave_parallelism=1 \
  data.adapter.tf.prefetch.final_stream_buffer=2 \
  data.adapter.tf.prefetch.per_dataset_stream_buffer=0 \
  data.adapter.tf.prefetch.episode_queue_buffer=0 \
  training.max_steps=2000 \
  training.max_epochs=-1 \
  training.validation.check_interval=500 \
  training.validation.limit_batches=1 \
  training.validation.num_sanity_val_steps=0 \
  training.validation.visualization.enabled=false \
  training.train_visualization.enabled=false \
  training.profiler.enabled=false \
  logging.use_wandb=true
```

### Worked

- Run: `/mnt/data/workspace/runs/hlrp/2026-02-18_22-05-43_local_s2_scale29_valcheck_bs32_python_v1_tmux`
  - Command diff vs shared family:
    - `experiment.name=local_s2_scale29_valcheck_bs32_python_v1_tmux`
    - `data.loader.batch_size=32`
  - Result: completed and exported artifact
    - `artifacts/smolvla_shared_stage2_artifact.pt`

- Run: `/mnt/data/workspace/runs/hlrp/2026-02-18_22-13-38_local_s2_scale29_valcheck_bs48_python_v2_tmux`
  - Command diff vs shared family:
    - `experiment.name=local_s2_scale29_valcheck_bs48_python_v2_tmux`
    - `data.loader.batch_size=48`
  - Result: completed and exported artifact
    - `artifacts/smolvla_shared_stage2_artifact.pt`

### Failed

- Run: `/mnt/data/workspace/runs/hlrp/2026-02-18_23-31-19_local_s2_scale29_bs48_auto_20260218_233118`
  - Command diff vs shared family:
    - `experiment.name=local_s2_scale29_bs48_auto_20260218_233118`
    - `data.loader.batch_size=48`
    - `training.checkpoint.every_n_train_steps=200`
    - `training.resume_from_checkpoint=/mnt/data/workspace/runs/hlrp/2026-02-18_22-20-19_local_s2_scale29_bs48_long_v1_tmux/checkpoints/last.ckpt`
  - Result: failed at startup during checkpoint restore
    - `RuntimeError: Error(s) in loading state_dict for VLATokenBackendLightningModule`
    - Cause: incompatible checkpoint/model state dict shape/keys.

### In progress / indeterminate (no explicit crash marker in `unified.log`)

- Run: `/mnt/data/workspace/runs/hlrp/2026-02-18_23-33-27_local_s2_scale29_bs48_auto_20260218_233325`
  - Command diff vs shared family:
    - `experiment.name=local_s2_scale29_bs48_auto_20260218_233325`
    - `data.loader.batch_size=48`
    - `training.checkpoint.every_n_train_steps=200`
    - no `training.resume_from_checkpoint` override
  - Last observed state in `unified.log`:
    - dataset info/loading and python-prefetch startup on the 29-dataset stream.
