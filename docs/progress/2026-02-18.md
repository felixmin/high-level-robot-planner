# Progress Update (2026-02-18)

## Stage-2 local param ladder (bs48, 10 steps)

Goal: isolate unstable/slow knobs by changing one parameter at a time from baseline.

Best result in this ladder:
- Run: `/mnt/data/workspace/runs/hlrp/2026-02-18_01-33-00_paramscan06_tfds_interleave2_bs48_s10`
- Override: `data.adapter.tf.tfds_read.interleave_parallelism=2`
- Throughput: `0.03897 steps/s` (`1.87050 samples/s`)

Confirmed bad settings:
- `data.adapter.tf.pipeline.interleave_parallelism=2` alone fails immediately (`num_parallel_calls=2` with `cycle_length=1`).
- `data.adapter.tf.pipeline.episode_concurrency=2` is valid but slower and higher RAM.

Detailed table is in:
- `docs/data/2026-02-17_local_s2_speed_iteration.md`

## Overnight run launched (tmux)

Session:
- `tmux` session name: `s2_overnight_20260218`

Command:
```bash
conda run -n hlrp python scripts/submit_job.py \
  experiment=vla_smol_flow_shared \
  cluster=local_dev \
  experiment.name=local_s2_overnight_bs48_steps25k_oxe_local_spe0_pscan_best \
  model.laq.checkpoint=/mnt/data/workspace/code/high-level-robot-planner/laq-stepstep052500.ckpt \
  data=oxe_local_spe0 \
  data.loader.batch_size=48 \
  training.max_steps=25000 \
  training.max_epochs=-1 \
  training.validation.check_interval=2000 \
  training.validation.visualization.enabled=true \
  training.train_visualization.enabled=true \
  training.profiler.enabled=false \
  data.adapter.tf.tfds_read.interleave_parallelism=2 \
  logging.use_wandb=true
```

Active run dir:
- `/mnt/data/workspace/runs/hlrp/2026-02-18_01-44-51_local_s2_overnight_bs48_steps25k_oxe_local_spe0_pscan_best`

Monitor:
```bash
tail -f /mnt/data/workspace/runs/hlrp/2026-02-18_01-44-51_local_s2_overnight_bs48_steps25k_oxe_local_spe0_pscan_best/unified.log
```

Check tmux:
```bash
tmux ls
tmux attach -t s2_overnight_20260218
```

## Check result (2026-02-18 morning)

Outcome:
- Run did not reach training steps and did not export any artifact/checkpoint.
- Process was OOM-killed by kernel.

Evidence:
- No active process for this run (`4_train_foundation.py` exited).
- Last `unified.log` entries are still TFDS dataset-construction lines (no `Epoch 0` / step metrics).
- Kernel log shows explicit kill:
  - `Out of memory: Killed process 1934864 (python) ... anon-rss:35627112kB`
  - timestamp: `Feb 18 08:14:10`
  - command was this run's stage-2 process in tmux scope.

## Cluster queue additions (forced GCS source)

Submitted extra comparison jobs with forced GCS source:
- `5486342` (`cluster_s2_bs48_match_local_v6_gcs_lrz2`)
- `5486341` (`cluster_s2_bs48_match_local_v6_gcs_mcml2`)

Current status snapshot:
- Running baseline: `5486298` (`cluster_s2_bs48_spe0_highram_v2_lrz`)
- Pending auto-source pair: `5486339`, `5486338`
- Pending forced-GCS pair: `5486342`, `5486341`

## Stage-2 diagnostics/viz patch

Implemented code updates to improve sample debugging and metrics visibility:
- Fixed val sample JSON behavior for `latent_flow`: keep `gt_codes` even when `pred_codes` is absent.
- Added dataset metadata to sample visualizations/JSON:
  - per-sample `dataset_name` in both train/val records
  - dataset shown in panel `meta` text
- Added richer validation stats in `VLATokenBackendLightningModule`:
  - `val/latent_vector_stats/*` and `val/action_stats/*`
  - includes mean/std, norm means, MSE/MAE, cosine mean, and quantized unique-count fractions
- Added validation batch dataset-mix stats:
  - `val/dataset_mix_unique`
  - `val/dataset_mix_top1_frac`
  - `val/dataset_mix_entropy_norm`
  - top dataset fractions as `val/dataset_mix_frac_<dataset>`
  - stdout line: `[Val][BatchMix] ...`
- Added periodic training-batch dataset composition logging:
  - callback prints `[Train][BatchMix] step=... datasetA=n ...`
  - controlled by `training.dataset_usage_logger.log_batch_composition_every_n_steps`

Tests:
- `tests/test_vla_visualization_callback.py`
- `tests/test_vla_train_visualization_callback.py`
- `tests/test_dataset_usage_logger_callback.py`

## Sync + quick smoke submissions (2026-02-18 evening)

- Local commit:
  - `f6f18c2` (`Add stage2 vector+dataset diagnostics and fix sample viz records`)
- Pushed to `origin/main` and pulled on cluster checkout:
  - cluster HEAD: `622e77a` (merge commit containing `f6f18c2`)

Short Stage-2 verification pair submitted (forced `source=gcs`):
- LRZ: `5486353`
  - run dir: `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-18_17-46-58_cluster_s2_bs48_diag20_gcs_lrz`
- MCML: `5486354`
  - run dir: `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-18_17-46-58_cluster_s2_bs48_diag20_gcs_mcml`

Config for both:
- `training.max_steps=20`
- `training.validation.check_interval=20`
- `training.validation.limit_batches=4`
- `training.validation.visualization.enabled=true`
- `training.train_visualization.enabled=true`
- `training.dataset_usage_logger.log_batch_composition_every_n_steps=5`
- `data.adapter.tf.tfds_read.source=gcs`
- `data.loader.batch_size=48`
- `+submit.time=00:15:00`

Note on queued jobs and updated code:
- `submit_job.py` mounts repo path directly and sets container workdir to that repo path (`scripts/submit_job.py`, see sbatch template section).
- Therefore queued jobs started after this pull use the updated cluster checkout (including the new diagnostics patch).

## Local validation OOM deep-dive (2026-02-18 late evening)

- Detailed note:
  - `docs/data/2026-02-18_local_stage2_validation_oom.md`

- Key results:
  - Reproduced local OOM during repeated python-mixer re-inits (`pid 2286800` killed by kernel at `18:28:40`).
  - Observed repeated log pattern:
    - `python-prefetch mixing: starting 29 workers`
    - `initial ready=1/29`
    - repeated cycles with rising RAM.
  - Implemented adapter fix in `packages/common/adapters/oxe.py`:
    - python-prefetch workers now use timeout-based queue put (`Full` retry) so they can exit when `stop` is set.
  - Post-fix behavior improved (less immediate runaway), but restart-loop pressure still exists for local 29-dataset python-mixer runs.

- Cluster status update:
  - `5486298` finished due `TIMEOUT` (not crash), reached validation at steps 2000 and 4000.
  - Pending queue remains:
    - `5486339`, `5486342`, `5486353`, `5486338`, `5486341`, `5486354`.
