# Progress Log - 2026-02-17

## Scope
Current workstream:
1. Stage 2 (SmolVLA shared latent-flow pretraining) on OXE.
2. Stage 3 (LeRobot fine-tune/eval) from Stage-2 artifact.
3. Local workstation iteration (RTX 5090) before longer cluster runs.

## Where We Stand
- Stage 3 `bs=48`, `steps=500`, W&B, eval rollouts: completed successfully after BF16 rollout fix.
- Successful run:
  - Run dir: `/mnt/data/workspace/runs/hlrp/2026-02-17_21-57-51_lerobot_hlrp_smolvla_shared_bs48_s500_eval_wandb_fixbf16`
  - W&B: `https://wandb.ai/felixmin/lerobot/runs/h6wg1dtq`
  - Key events: reached step 500, checkpointed, eval at steps 250 and 500, end-of-training.
- Prior run failed at eval due to BF16 -> NumPy conversion:
  - Run dir: `/mnt/data/workspace/runs/hlrp/2026-02-17_21-51-19_lerobot_hlrp_smolvla_shared_bs48_s500_eval_wandb`
  - W&B: `https://wandb.ai/felixmin/lerobot/runs/krmh8u1x`
  - Error: `TypeError: Got unsupported ScalarType BFloat16`.
- Fix applied:
  - `lerobot_policy_hlrp/src/lerobot_policy_hlrp/policies/hlrp_smolvla_shared/modeling_hlrp_smolvla_shared.py`
  - `predict_action_chunk` now returns `float32` actions for rollout boundary.

## Exact Commands Used (Stage 3)
### Failed run (before BF16 rollout fix)
```bash
conda run -n lerobot python scripts/6_train_lerobot.py \
  experiment=lerobot_hlrp_smolvla_shared_smoke \
  cluster=local_dev \
  logging.root_dir=/mnt/data/workspace/code/high-level-robot-planner \
  experiment.name=lerobot_hlrp_smolvla_shared_bs48_s500_eval_wandb \
  lerobot.stage2_artifact=/mnt/data/workspace/runs/hlrp/2026-02-17_21-26-36_vla_smol_flow_shared_stage2_short_synth_bs48/artifacts/smolvla_shared_stage2_artifact.pt \
  lerobot.policy_device=cuda \
  lerobot.steps=500 \
  lerobot.batch_size=48 \
  lerobot.num_workers=0 \
  lerobot.eval_freq=250 \
  lerobot.eval_batch_size=1 \
  lerobot.log_freq=10 \
  lerobot.save_freq=1000 \
  logging.use_wandb=true \
  lerobot.wandb_enable=true \
  lerobot.extra_args='["--eval.n_episodes=2"]'
```

### Successful run (after BF16 rollout fix)
```bash
conda run -n lerobot python scripts/6_train_lerobot.py \
  experiment=lerobot_hlrp_smolvla_shared_smoke \
  cluster=local_dev \
  logging.root_dir=/mnt/data/workspace/code/high-level-robot-planner \
  experiment.name=lerobot_hlrp_smolvla_shared_bs48_s500_eval_wandb_fixbf16 \
  lerobot.stage2_artifact=/mnt/data/workspace/runs/hlrp/2026-02-17_21-26-36_vla_smol_flow_shared_stage2_short_synth_bs48/artifacts/smolvla_shared_stage2_artifact.pt \
  lerobot.policy_device=cuda \
  lerobot.steps=500 \
  lerobot.batch_size=48 \
  lerobot.num_workers=0 \
  lerobot.eval_freq=250 \
  lerobot.eval_batch_size=1 \
  lerobot.log_freq=10 \
  lerobot.save_freq=1000 \
  logging.use_wandb=true \
  lerobot.wandb_enable=true \
  lerobot.extra_args='["--eval.n_episodes=2"]'
```

## Data/Caching Notes (Why dataset worked)
- LeRobot dataset cache exists locally and was reused:
  - `/home/felix/.cache/huggingface/lerobot/HuggingFaceVLA/libero`
- For smoke runs we used `dataset.episodes=[0]`, so only a small subset is required.
- LIBERO assets cache:
  - `/home/felix/.cache/libero/assets`

## Monitoring / How To Reach Runs
- List latest run dirs:
```bash
ls -1dt /mnt/data/workspace/runs/hlrp/* | head
```
- Tail unified launcher log:
```bash
tail -f /mnt/data/workspace/runs/hlrp/<timestamp>_<experiment>/unified.log
```
- Tail LeRobot/W&B captured output:
```bash
tail -f /mnt/data/workspace/runs/hlrp/<timestamp>_<experiment>/lerobot_smolvla_shared_smoke/wandb/run-*/files/output.log
```

## Planned Overnight Stage 2 Run (updated for ~8h)
Reasoning: `15k` is likely low for an ~8h target. Based on current local throughput, start with `25k` steps and re-estimate after first 1-2k steps.

Requested validation cadence:
- Stage 2 validation interval: `2000` steps.

Planned command:
```bash
conda run -n hlrp python scripts/submit_job.py \
  experiment=vla_smol_flow_shared \
  cluster=local_dev \
  experiment.name=local_s2_overnight_bs48_steps25k_oxe_local_spe0 \
  model.laq.checkpoint=/mnt/data/workspace/code/high-level-robot-planner/laq-stepstep052500.ckpt \
  data=oxe_local_spe0 \
  data.loader.batch_size=48 \
  training.max_steps=25000 \
  training.max_epochs=-1 \
  training.validation.check_interval=2000 \
  training.validation.limit_batches=10 \
  training.validation.num_sanity_val_steps=0 \
  logging.use_wandb=true
```

Expected Stage-2 artifact path:
- `/mnt/data/workspace/runs/hlrp/<timestamp>_local_s2_overnight_bs48_steps25k_oxe_local_spe0/artifacts/smolvla_shared_stage2_artifact.pt`

## Next Stage 3 Run From New Artifact
Requested validation cadence:
- Stage 3 eval interval: `10000` steps.

Planned command template:
```bash
conda run -n lerobot python scripts/6_train_lerobot.py \
  experiment=lerobot_hlrp_smolvla_shared_smoke \
  cluster=local_dev \
  logging.root_dir=/mnt/data/workspace/code/high-level-robot-planner \
  experiment.name=local_s3_from_s2_overnight_eval10k \
  lerobot.stage2_artifact=/mnt/data/workspace/runs/hlrp/<stage2_run>/artifacts/smolvla_shared_stage2_artifact.pt \
  lerobot.policy_device=cuda \
  lerobot.steps=20000 \
  lerobot.batch_size=48 \
  lerobot.num_workers=0 \
  lerobot.eval_freq=10000 \
  lerobot.eval_batch_size=1 \
  lerobot.log_freq=10 \
  lerobot.save_freq=10000 \
  logging.use_wandb=true \
  lerobot.wandb_enable=true \
  lerobot.extra_args='["--eval.n_episodes=1"]'
```

Note on rollout count:
- `libero_goal` is a 10-task suite.
- `--eval.n_episodes=1` means ~10 total rollouts per eval (1 per task).
- `--eval.n_episodes=10` means ~100 total rollouts per eval (10 per task), much slower.

## Open Decisions Before Launch
1. Stage 2 step target for tonight:
   - default in this plan: `25000` (recommended)
   - alternatives: `22000` (safer), `30000` (aggressive)
2. Stage 3 rollout budget interpretation:
   - choose `eval.n_episodes=1` (~10 total rollouts) or `10` (~100 total rollouts).
3. Keep Stage 2 train/val visualizations at current defaults or disable for max throughput.

## Decisions Locked + Launch Status
- Confirmed:
  - Stage 2 steps: `25000`
  - Stage 3 rollout budget: `eval.n_episodes=1` (about 10 total rollouts per eval on `libero_goal`)
  - Stage 2 validations/visualizations: enabled
- Overnight Stage 2 run launched:
  - Run dir: `/mnt/data/workspace/runs/hlrp/2026-02-17_22-41-17_local_s2_overnight_bs48_steps25k_oxe_local_spe0`
  - Command (launched via detached `tmux`):
```bash
conda run -n hlrp python scripts/submit_job.py \
  experiment=vla_smol_flow_shared \
  cluster=local_dev \
  experiment.name=local_s2_overnight_bs48_steps25k_oxe_local_spe0 \
  model.laq.checkpoint=/mnt/data/workspace/code/high-level-robot-planner/laq-stepstep052500.ckpt \
  data=oxe_local_spe0 \
  data.loader.batch_size=48 \
  training.max_steps=25000 \
  training.max_epochs=-1 \
  training.validation.check_interval=2000 \
  training.validation.visualization.enabled=true \
  training.train_visualization.enabled=true \
  logging.use_wandb=true
```
- Quick monitor:
```bash
tail -f /mnt/data/workspace/runs/hlrp/2026-02-17_22-41-17_local_s2_overnight_bs48_steps25k_oxe_local_spe0/unified.log
```

## Cluster Sync + Matching Stage-2 Submission
- Local commit pushed:
  - `ee8d9d4` (`main`)
- Cluster repo synced (via `git pull --no-rebase` on `ssh ai` checkout):
  - head now `47a9692` (merge including `ee8d9d4`)
- Cluster job submitted with same performance-related overrides as local run, adapting only environment/data:
  - `cluster=lrz_x100`
  - `data=oxe_cluster_spe0`
  - `model.laq.checkpoint=/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/2026-02-11_laq_80k.ckpt`

Exact submitted command:
```bash
python scripts/submit_job.py \
  experiment=vla_smol_flow_shared \
  cluster=lrz_x100 \
  experiment.name=cluster_s2_bs48_steps25k_oxe_cluster_spe0 \
  model.laq.checkpoint=/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/2026-02-11_laq_80k.ckpt \
  data=oxe_cluster_spe0 \
  data.loader.batch_size=48 \
  training.max_steps=25000 \
  training.max_epochs=-1 \
  training.validation.check_interval=2000 \
  training.validation.visualization.enabled=true \
  training.train_visualization.enabled=true \
  logging.use_wandb=true
```

Cluster submission result:
- Job ID: `5485027`
- State on submit check: `PENDING (Priority)`
- Run dir:
  - `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-17_22-48-26_cluster_s2_bs48_steps25k_oxe_cluster_spe0`
- Monitor:
```bash
squeue -j 5485027 -o "%.18i %.30P %.20j %.8T %.10M %.9l %R"
tail -f /dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-17_22-48-26_cluster_s2_bs48_steps25k_oxe_cluster_spe0/5485027.out
```

## Dual-Queue Submissions (LRZ + MCML) - 2026-02-17 22:50
Policy followed: submit both LRZ and MCML, then cancel the one that starts later.

### Full overnight pair (same performance config)
- LRZ: `5485027`
  - run dir: `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-17_22-48-26_cluster_s2_bs48_steps25k_oxe_cluster_spe0`
- MCML: `5485032`
  - run dir: `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-17_22-50-41_cluster_s2_bs48_steps25k_oxe_cluster_spe0_mcml`

### Fast verification pair (20 steps, 15 min)
- LRZ: `5485033`
  - run dir: `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-17_22-50-42_cluster_s2_bs48_steps20_15m_oxe_cluster_spe0_lrz`
- MCML: `5485034`
  - run dir: `/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-02-17_22-50-42_cluster_s2_bs48_steps20_15m_oxe_cluster_spe0_mcml`

Monitor:
```bash
ssh ai 'squeue --me -o "%.18i %.30P %.30j %.8T %.10M %.9l %R" | egrep "5485027|5485032|5485033|5485034|JOBID"'
```

Cancel the slower duplicate once one starts:
```bash
# full pair
ssh ai 'scancel <5485027_or_5485032>'
# short pair
ssh ai 'scancel <5485033_or_5485034>'
```
