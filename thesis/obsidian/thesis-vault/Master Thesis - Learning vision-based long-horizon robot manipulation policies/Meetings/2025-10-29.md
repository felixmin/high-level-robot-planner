---
notion-id: 29a20c92-0436-80dc-930a-f343867af78b
base: "[[Meetings.base]]"
Tags: []
Created: 2025-10-28T11:30:00
---
## Progress

- Further literature research
    - Gondola
- Started implementing VQ-VAE
- Experimented with duplo and chatgpt [[ChatGPT Planning Test]] 

## Thoughts

- Takeaways from experiments
→ Single camera pure obs + instr might not be sufficient
- What if action latents don’t work? Can we have or should we try multimodal outputs?
    - language + mask + action latents ?
    - Gondola also just calls the model with 

## Next steps

- Register thesis
- Try more sophisticated test of existing VLMs
    - Providing history and specific instructions to the model
    - Trying different models
- Install and test isaac sim on workstation
- Run LAPA and try to overfit with our data
- (Run first tests with VQ-VAE)


Was ist das problem das wir lösen wollen?

Was sind die constraints (e.g. camera cangle, …, hardware setup)? Data?

→ Trainingspipeline und deployment pipeline durchdenken

Wahrscheinlich Ziel: long horzion + precise manipulation 

Constraints:

- External camera
+ wrist camera ? For high-level policy the external might be enough… for low level this is more critical
Could only wrist camera work aswell? → 
- maybLarge vision and e sim data and small real data
- 


- Do we use object centric laq with no effectors in the image or with effectors?
    - What is the time horizon?
    - What changes without effectors?