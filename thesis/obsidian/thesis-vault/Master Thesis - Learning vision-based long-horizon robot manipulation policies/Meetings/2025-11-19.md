---
notion-id: 2b020c92-0436-808e-8fb9-fa7e5e55acd0
base: "[[Meetings.base]]"
Tags: []
Created: 2025-11-19T10:50:00
---
## Progress

- Deeply read through LAQ model code and surrounding theory
- Planned repository setup
- Thought about motion track usage

## Thoughts

- Use various decoders for leveraging motion tracks?
→ Latent actions should in theory already contain the information to show motion tracks
→ We can use this decoder 1. to visualize those latent actions better and 2. to add more training data 

## Questions

- 15min
- proposal slides
- motivation, key reserach gap, 3-5 most relevant related work, contribution, rough idea of method, preliminary results
    - lapa works
    - problems
    - example motion tracks overfitted → can work e.g. 10% better
    - what is a final demo (if i have a video what would it show?)
→ important: have high level motivation → robots want to have a world understanding → we have large scale video data → how can we leverage that
demo very concrete
e.g. 30 bricks
    - related work: from relevant people and 30s-1min per paper max
    - focus on visuals
    - rt trajectory image include
    - maybe one major research question could be which is the “best representation” for learning from videos? → part of method or contribution
→ we can not find the best probably, finding better represenattions
    - 


Co-tracker

peek


idea

Prio probieren