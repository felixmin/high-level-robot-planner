---
notion-id: 29420c92-0436-806f-a13b-c2605632ab6f
---
**
The Optimal Intermediate Goal Representation for Hierarchical Robot Control Architectures**

**
I. Foundational Principles: The Hierarchical Bottleneck in Embodied AI**

**
1.1. The Necessity of Hierarchy and Architectural Decoupling**

The execution of complex, long-horizon tasks, such as those encountered in domestic or manufacturing environments, necessitates planning across immense time scales and continuous geometric state spaces.1 To manage this inherent complexity, hierarchical architectural design has become paramount in contemporary robotics. This structure effectively partitions concerns: the high-level policy (often a Vision-Language Model or VLM) focuses on abstract semantic reasoning—the *what* of the task—while the low-level policy (Reinforcement Learning or Imitation Learning) focuses on precise, continuous geometric control—the *how* of the execution.2
This architectural decoupling yields substantial engineering and computational benefits. By separating low-frequency semantic planning from high-frequency, continuous control (which typically operates at over 100 Hz), the system optimizes computational efficiency and throughput.3 Furthermore, modularity allows researchers and engineers to perform separate, targeted training and evaluation. The VLM's ability to generate a correct sequence of steps can be assessed independently of the low-level policy's precision in grounding those steps into physical actions.4 This clear delineation of responsibilities is crucial for scaling robotic systems to handle diverse and complex workloads.
**
1.2. Defining the Intermediate Interface: The Trade-off of Grounding**

The critical challenge in hierarchical control lies in defining the intermediate representation—the "well defined interface layer" 5—that effectively bridges the high-level semantic plan with the low-level policy execution. This interface must serve as a communication channel, translating abstract user intent into grounded, dense, and actionable conditioning signals for the downstream RL or IL policy.
The design of this representation involves navigating competing constraints. It must be semantically rich enough to be interpretable by the VLM (for accurate planning) and potentially by a human operator (for easy modification or diagnosis).4 Simultaneously, it must possess robust geometric and physical expressivity to provide the necessary continuous parameters for the low-level policy to achieve high-precision control. For policies based on RL or IL, the structure of this conditioning signal fundamentally dictates learning efficiency. If the goal signal is sparse, brittle, or requires extensive interpretation in the observation space (like pixels), the policy training suffers severely from high sample inefficiency.7 Therefore, the architectural design must prioritize the requirements of the low-level policy. Since RL and IL objectives are optimized by maximizing a reward function, the interface must naturally lend itself to providing **dense, high-quality conditioning signals**. Historical success in goal-conditioned reinforcement learning (GCRL) demonstrates that defining goal distance metrics within a compact, learned latent space is inherently superior to using sparse linguistic or pixel-based inputs for achieving rapid and robust policy learning.7 This suggests that regardless of the VLM's output format, the signal ultimately handed to the continuous policy must resolve into a dense, continuous latent vector.
**
II. High-Level Planning Modalities: The VLM’s Role in Structured Decomposition**

**
2.1. VLM/LLM as Zero-Shot and Few-Shot Task Decomposers**

The recent widespread adoption of Vision-Language Models (VLMs) and Large Language Models (LLMs) in robotics stems from their superior semantic understanding and generalization capabilities, inherited from vast internet-scale pre-training.9 This pre-training enables strong zero-shot or few-shot generalization, significantly reducing the reliance on costly, task-specific robot demonstration data.4 VLMs allow high-level planners to reason about the open world and complex, long-horizon objectives.10
However, the effective utilization of VLMs for reliable high-level planning requires careful structuring of inputs and outputs. VLMs must be guided toward reliable plans through structured prompting techniques, such as using Visual Question Answering (VQA) formats 4 or providing memory mechanisms to handle non-Markovian state tracking (crucial for tasks like object swapping).4 These techniques refine the VLM’s internal representation of the task, improving its planning accuracy before the output is generated.6
**
2.2. The Shift to Structured Code and API Calls**

Contemporary research shows a decisive movement toward utilizing code generation as the preferred modality for VLM planning output. In this paradigm, the VLM is trained to decompose a high-level task description into a sequence of "executable subroutines" or API code.4
This code-based approach provides a crucial structural benefit: it treats open-source robotic APIs not just as execution endpoints but as sources of **structured supervision**.4 The associated subtask functions become modular, semantically meaningful labels that aid the training of the VLM via supervised fine-tuning (SFT) or imitation learning (IL). This structured format facilitates interpretability of the plan decomposition.4 For instance, a complex command is broken down into discrete symbolic steps like `move_to(object_A, location_B)` or `insert_peg(parameter_C)`, where each symbolic function corresponds to a known low-level skill.
To ensure the plan is executable and unambiguous, it is essential to enforce syntactic correctness in the VLM's output. Techniques such as structured output generation—leveraging tools like vLLM with `guided_json` or `guided_grammar` backends—constrain the VLM’s output to a predefined set of syntax rules and arguments.12 This process translates the abstract reasoning of the VLM into a programmatically valid sequence of steps, effectively defining the discrete skill choices and the continuous parameters for those skills. The resulting VLM output is inherently a **symbolic or discrete representation**, selecting one primitive from a predefined skill library and instantiating it with parameters.2 This architecture establishes the VLM’s role as answering *which skill to use* and *with what parameters*, delegating the continuous physical execution responsibility entirely to the low-level policy.
**
2.3. VLM-Diffusion Architectures for Grounding**

A potent realization of this hierarchical approach involves coupling VLM code generation with modern diffusion policies. In frameworks such as the Hierarchical Learning of Diffusion-VLM Policies, the VLM generates the API code, and this code serves as the conditioning signal for a low-level diffusion policy.4
The diffusion policy, often a conditional diffusion model, is then trained to predict low-level actions based on this code instruction. This design leverages the VLM's reasoning capabilities while ensuring the low-level controller benefits from the robustness and expressiveness of diffusion models for continuous action prediction. In this context, the code acts as an "intermediate representation" used for learning both high-level and low-level neural policies.4 This paradigm successfully utilizes the VLM for abstract, discrete reasoning and offloads the challenge of continuous control to a specialized, data-efficient low-level model.
**
III. Comparative Evaluation of Low-Level Conditioning Modalities**

The choice of intermediate representation significantly impacts the sample efficiency and generalization of the low-level policy. A critical evaluation of common modalities reveals distinct trade-offs regarding semantic power versus grounded fidelity.
**
3.1. Modality 1: Natural Language Commands (Pure Text Interface)**

Using natural language directly as the interface means the high-level VLM outputs a text instruction (e.g., "pick up the red block and place it on the shelf"), which directly conditions the low-level policy. While this offers high interpretability and leverages the LLM's vast knowledge, it suffers from several critical limitations.
Firstly, natural language has an inherent expressivity ceiling. Not all tasks, particularly those requiring fine-grained motor control or complex, non-linguistic geometric operations (such as performing a specific torque-controlled insertion or a precise dance routine), can be easily or accurately decomposed into simple text steps.5 Secondly, pure language tokens create architectural difficulties during fine-tuning on embodied data. Attempts to perform end-to-end tuning on language-conditioned policies often lead to a domain shift or catastrophic forgetting, damaging the embedding space of the word tokens learned during large-scale pre-training.5 Empirical evidence supports these concerns: baselines relying on pure language interfaces, even when utilizing powerful proprietary models like GPT-4V, consistently demonstrate inferior performance compared to hybrid approaches on tasks requiring sustained reasoning and multi-step execution.14
**
3.2. Modality 2: Visual and Image Goals (Pixel Space)**

Visual goals, where the desired outcome is specified as a target observation image ($G_{img}$), are intuitive for humans and useful for demonstrating a specific final state. However, they present significant challenges when used as the goal condition for model-free RL policies.8
The primary obstacle is the difficulty in defining a dense, meaningful reward function in pixel space. Direct pixel-based distance metrics are sparse, brittle, and highly sensitive to irrelevant visual variations (e.g., lighting changes or background clutter).7 Policy training struggles immensely with sample efficiency because the RL agent must explore extensively to cover the state space sufficiently to learn the visual distance metric reliably.7 Pioneering work in Visual Goal-Conditioned RL (GCRL) established that pixel-based rewards are substantially outperformed when the reward function is instead defined using the distance metric derived from a learned Variational Autoencoder (VAE) latent space.7 This historical finding established the necessity of moving beyond raw visual input for efficient policy conditioning.
**
3.3. Modality 3: Learned Latent Codes (The Latent Bridge Paradigm)**

Learned latent codes represent a dedicated approach to solving the interface problem. Here, the intermediate goal is communicated as a compact, dense vector $z_{goal}$, generated either by an encoder trained to map symbolic plans to the policy space (e.g., Learnable Latent Codes as Bridges, or LCB) 5 or by compressing the visual changes between initial and goal states (e.g., Image-GOal Representations, or IGOR).15
The core advantage of latent codes is their ability to act as a **flexible bridge**.5 This intermediate representation allows the VLM to communicate high-level goals without its underlying word token embeddings being fundamentally constrained or destroyed during end-to-end fine-tuning on robot data.5 This is critical for maintaining the VLM's hard-earned generalization capabilities while permitting robust supervised training of the downstream policy. Furthermore, latent codes are computationally efficient. They allow for the forecasting of dense trajectories in a compact, learned space, which scales gracefully to vision-based domains without the heavy computational burden of video generation in pixel space.16 Empirical results validate this approach, with methods like LCB exhibiting superior competence and heightened success rates over pure language interface baselines on complex, long-horizon tasks requiring reasoning.14
**
3.4. Modality 4: Physical and Geometric Primitives/Constraints**

While latent codes provide the optimal numerical input for learning, the ultimate representation of a low-level goal must be tied to physical and geometric quantities. This modality involves direct specification of desired physical outcomes, such as end-effector poses, accelerations, joint commands, or dynamic constraints like target contact forces.17
The use of explicit physical parameters is non-negotiable for high-precision, contact-rich tasks like robotic assembly or compliant interaction (e.g., shaving).2 Low-level control schemes, including Model Predictive Control (MPC) and optimal control methods like Linear Quadratic Control (LQC), intrinsically rely on these explicit, continuous feature goals.19 In the context of a hierarchical system, these physical values define the **continuous input parameters** necessary to instantiate the chosen low-level skill. The high-level VLM determines the symbolic skill (`insert`) and specifies the continuous parameters (e.g., spatial coordinates, target force level) that define the desired trajectory.1 Thus, the latent code must effectively encode these physical parameters to ground the high-level plan successfully.
**
IV. Synthesis and Architectural Recommendations: The Structured Latent Code**

**
4.1. Recent Trends in Embodied AI Goal Representation (2022–2025)**

Analysis of recent robotics literature reveals a clear convergence on hybrid, modular architectures, moving away from seeking a single, monolithic goal modality. The core focus is on efficiently transferring the robust semantic understanding of foundational models to the geometric precision required for physical embodiment. The emerging architectural consensus can be summarized as a flow: Semantic Planner $\rightarrow$ Latent Interface $\rightarrow$ Grounded Controller.
The trends indicate a strong focus on utilizing VLMs for symbolic, structured outputs 4 and relying on compact latent spaces to bridge the semantic-geometric gap.5
Table 1: Recent Trends in Hierarchical Policy Interface Design (2022-Present)**Trend DescriptionObserved Shift/FocusKey Architectural ConceptCiting LiteratureFoundation Model Adoption**Leveraging pre-trained LLMs/VLMs for complex, open-world task planning.VLM Code Generation, VLM2VLA, Project GR00T.4**Hybrid Latent Bridging**Prioritizing a learned, compact latent space for robust policy communication over pure language.Latent Codes as Bridges (LCB), unified latent action spaces (IGOR).5**Modular Skill Composition**Combining specialized, high-precision low-level skills (RL/Model-Based) via a high-level IL planner.Primitive libraries, Adaptive Robotic Compositional Hierarchy (ARCH).2**Diffusion Policy Grounding**Utilizing diffusion models for robust, conditional action generation in the low-level.Code-conditioned Diffusion Policies.4
**
4.2. Recommendation: The Parametric Latent Code Interface ($\boldsymbol{z_{PLC}}$)**

For the system described—leveraging a pre-trained VLM for high-level planning and an RL/IL policy for low-level execution—the most robust, efficient, and generalizable interface is the **Parametric Latent Code ($z_{PLC}$)**. This representation is not a pure latent code but a highly structured vector that compactly encodes the symbolic skill choice and all associated continuous physical parameters.
This approach resolves the dual modality imperative by defining a precise architectural flow:
1. Macro-Level Plan (VLM Output - Modality: Structured Code): The VLM's role is to act as a structured code generator, decomposing the natural language task into a sequence of API calls. The output should be a constrained, machine-readable format (like JSON) specifying the discrete skill ID and its continuous geometric or physical parameters.12
$$ \text{VLM Output} \rightarrow {\text{skill_ID}, \text{primitive_params}: [x, y, z, \text{Force}]} $$
2. **The Interface (Latent Bridge - Modality: Learned Latent Vector):** A dedicated, lightweight "Latent Bridge" network (drawing inspiration from LCB) encodes this structured, multi-modal input (the code structure plus relevant visual observations) into the dense, compact vector $z_{PLC}$. This bridge training ensures semantic transfer without retraining the large VLM, maintaining architectural stability.5
3. **Micro-Level Execution (Policy Input - Modality: Learned Latent Code):** The low-level RL/IL policy is conditioned solely on $z_{PLC}$. This compact vector contains all necessary information to execute the specific skill with the specified continuous physical goal parameters.
**
4.3. Detailed Implementation Plan for Low-Level Grounding**

The effectiveness of the $z_{PLC}$ hinges on its utility in optimizing the low-level policy.
**1. Skill Library and Parameterization:** The foundation requires a well-defined library of low-level primitive skills (e.g., grasping, insertion, navigation, trajectory following), as employed in frameworks like ARCH.2 Crucially, each primitive must be parameterized by the continuous inputs derived from the high-level plan (e.g., spatial coordinates, specific joint angles, or contact forces).2
**2. Training the Latent Bridge and Policy Conditioning (IL/SFT):** The Latent Bridge is trained, often via supervised learning, to minimize the compression error of mapping the structured API call and current state into $z_{PLC}$. This training ensures that $z_{PLC}$ is discriminative and contains all the necessary physical information for action prediction. The low-level policy (whether an IL-based Diffusion Model 4 or an RL policy) is then trained via behavior cloning (IL) or supervised fine-tuning (SFT), conditioned by $z_{PLC}$ corresponding to the execution of the target skill. This methodology effectively teaches the policy how to generalize across the continuous parameter space of the skills.
**3. Online Learning and Reinforcement Learning Integration:** If online learning or RL refinement is desired, the $z_{PLC}$ provides the ideal goal condition for GCRL. By defining the reward function $R(s, a, g)$ based on the distance between the latent goal $z_{PLC}$ and the latent embedding of the resulting state $\Phi(s')$, the system leverages dense reward signals:$$R(s, a, g) = -\Vert \Phi(s') - z_{PLC} \Vert_2$$
This structure ensures that the policy receives a continuous, dense learning signal that guides it toward the goal state, far surpassing the efficiency of sparse environment rewards or brittle pixel-based distances.7 This capacity for dense reward computation within the latent space is essential for overcoming the sample inefficiency common in real-world robotic RL applications.
The utilization of the $z_{PLC}$ as the interface significantly enhances the system's robustness and diagnosability through functional decomposition. By maintaining a structured boundary between abstract reasoning and continuous control, system failures become localized. If the high-level VLM selects the wrong symbolic skill (semantic error), the failure is confined to the planning module, as evidenced by the generated code. If the VLM plan is correct but the robot fails to execute the action precisely, the failure is localized to the low-level policy's ability to ground the continuous latent parameters $z_{PLC}$ (execution error). This clean separation facilitates precise diagnosis and permits targeted retraining of the deficient module—either adjusting the VLM's structured supervision or fine-tuning the low-level policy on challenging parameters.4 In contrast, a pure language goal would merge these errors, making systemic failures opaque and hindering sample-efficient refinement. The $z_{PLC}$ thus acts as a vital structuring mechanism, ensuring that the VLM's broad semantic knowledge is leveraged effectively while its output is rigorously grounded for physical execution.