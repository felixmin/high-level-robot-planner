---
notion-id: 2f020c92-0436-8000-9029-f2f4b241eba7
---
LAPA

VIPRA

pi 0.5


gr00t

cosmos reason 2

vqbet

openvla

> [!note]+ vipra vs lapa
> 
> !![[image 5.png]]

> [!note]+ vipra vs lapa detailed code comparison
> 1. Configuration (Action Definition)
> Both inherit from LLaMAConfig, but diverge in how they define the action space.
>     - VIPRA (Continuous): Defines dimensions for continuous vectors.
>         - File: vipra/vipra/policy/dynamics_action_cont_llama.py
> 1     class VideoLLaMAConfig(LLaMAConfig):
> 2         def **init**(self, ..., action_dims=8, action_chunk_size=14, ...):
> 3             # ...
> 4             self.action_dims = action_dims  # Continuous vector size
> 5             self.action_chunk_size = action_chunk_size
>     - LAPA (Discrete): Defines a vocabulary size for tokenized actions.
>         - File: LAPA/latent_pretraining/delta_llama_action.py
> 1     class VideoLLaMAConfig(LLaMAConfig):
> 2         def **init**(self, ..., action_vocab_size=245, ...):
> 3             # ...
> 4             self.action_vocab_size = action_vocab_size # Discrete vocab size
> 2. Model Setup (Input Embeddings)
> This is where the architecture splits most significantly. VIPRA uses an MLP for embeddings; LAPA uses a lookup table.
> - VIPRA (MLP + Time Embedding):
>     - File: vipra/vipra/policy/dynamics_action_cont_llama.py (Inside FlaxVideoLLaMAModule.setup)
> 
> 1     # Replaces standard embedding with a Flow Matching network
> 2     self.time_emb = SinusoidalPosEmb(self.config.hidden_size // 2)
> 3     self.action_encoder = ActionEncoder(
> 4         self.config.action_dims,
> 5         self.config.hidden_size,
> 6         time_cond=True,
> 7         ...
> 8     )
> 
> - LAPA (Standard Embedding):
>     - File: LAPA/latent_pretraining/delta_llama_action.py (Inside FlaxVideoLLaMAModule.setup)
> 
> 1     # Standard lookup table for action tokens
> 2     self.ate = nn.Embed(
> 3         self.config.action_vocab_size,
> 4         self.config.hidden_size,
> 5         ...
> 6     )
> 
> 3. Forward Pass (Processing Inputs)
> How actions are injected into the transformer stream.
> - VIPRA (Encode Continuous Values):
>     - File: vipra/vipra/policy/dynamics_action_cont_llama.py (Inside FlaxVideoLLaMAModule.**call**)
> 
> 1     # Takes raw 'actions' and 'time' as input args
> 2     noisy_action_embeds = self.action_encoder(actions, self.time_emb(time))
> 3
> 4     # Mixes embeddings based on masks
> 5     input_embeds = input_embeds * (1 - action_masks) + noisy_action_embeds * action_masks
> 
> - LAPA (Lookup Discrete Tokens):
>     - File: LAPA/latent_pretraining/delta_llama_action.py (Inside FlaxVideoLLaMAModule.**call**)
> 
> 1     # Takes 'input_ids' (indices) as input
> 2     input_action_embeds = self.ate(jnp.where(action_masks, input_ids, 0))
> 3
> 4     # Mixes embeddings based on masks
> 5     input_embeds = input_embeds * (1 - action_masks) + input_action_embeds * action_masks
> 
> 4. Output Heads (Prediction)
> - VIPRA (Regression): Outputs a vector of size action_dims.
>     - File: vipra/vipra/policy/dynamics_action_cont_llama.py
> 1 self.action_head = nn.Dense(
> 2 self.config.action_dims, # 8
> 3 use_bias=True,
> 4 ...
> 5 )
> - LAPA (Classification): Outputs logits over action_vocab_size.
>     - File: LAPA/latent_pretraining/delta_llama_action.py
> 
> 1     self.action_head = nn.Dense(
> 2         self.config.action_vocab_size, # 245
> 3         use_bias=False,
> 4         ...
> 5     )
> 
> 5. Generation (Inference Loop)
> This is the most drastic functional difference.
> - VIPRA (ODE Solver): Implements Flow Matching inference.
>     - File: vipra/vipra/policy/dynamics_action_cont_llama.py
>     - Function: generate_action_naive (or generate_action_kv_cache)
> 
> 1     # Explicit Euler integration loop
> 2     def euler_step(state, _):
> 3         current_action, current_time, key = state
> 4         vel = model_step(current_action, current_time)
> 5         # Update action based on velocity field
> 6         updated = current_action[:, condition_length:, :] + delta_t * vel
> 7         # ...
> 
> - LAPA (Autoregressive Sampling): Standard next-token prediction.
>     - File: LAPA/latent_pretraining/delta_llama_action.py
>     - Function: _sample_vision (or inherited _sample)
> 
> 1     # Standard categorical sampling
> 2     next_token = jax.random.categorical(prng_key, logits, axis=-1)
> 3
> 4     # Appends token to sequence
> 5     next_sequences = lax.dynamic_update_slice(state.sequences, next_token, (0, state.cur_len))

> [!note]+ vipra tokenization
> 1. Simultaneous Training Objectives
> In vipra/vipra/policy/train.py (specifically within the train_step function around line 386 for the vision,text,latent_action,latent_state,action_cont modality), the model computes losses for multiple heads:
> - Continuous Actions (Flow Matching):
>     - It samples a random time step time and noise noise.
>     - It creates noisy_actions by interpolating between the ground truth actions and noise.
>     - The model predicts the flow pred_flow (velocity).
>     - Loss: action_loss is the L2 distance (Mean Squared Error) between the predicted flow and the ground truth flow (gt_flow).
> 1 action_loss = l2_loss(pred_flow, gt_flow, batch["action_masks"])
> - Discrete Latent Actions (Classification):
>     - The model outputs latent_action_logits.
>     - Loss: latent_action_loss is the Cross-Entropy Loss against the ground truth latent action tokens (target_tokens).
> 
> 1     latent_action_loss, latent_action_acc = cross_entropy_loss_and_accuracy(
> 2         latent_action_logits,
> 3         jnp.where(batch["target_latent_action_masks"], batch["target_tokens"], 0),
> 4         ...
> 5     )
> 
> - Latent States (Classification):
>     - Similarly, it computes latent_state_loss (Cross-Entropy) for predicting latent states.
> 2. The Combined Loss
> The final loss used for backpropagation combines these individual losses. Interestingly, for the continuous action case (action_cont), the code snippet I read shows:
> 
> 1 loss = action_loss + latent_state_loss
> 
> Note: In the snippet I read, `latent_action_loss` was calculated but seemingly not added to the final `loss` variable for this specific `elif` block, unlike `latent_state_loss`. This might be a specific configuration choice
> for that experiment (e.g., pre-training latent actions separately or freezing them), or it effectively treats them as auxiliary tasks if they are added elsewhere or if I missed a line. However, the mechanism is clear: it
> computes distinct losses for continuous regression and discrete classification and optimizes the shared transformer backbone to satisfy both.
> 
> 3. Inference Usage
> During inference (in sampler_dynamics_action_cont.py), the model can be switched between modes:
> - Action Generation: It uses the flow matching sampler (generate_action_kv_cache) to iteratively solve the ODE and produce continuous actions.
> - Latent State/Action Generation: It can standard autoregressive sampling (generate with sample_mode="latent_state") to predict the discrete tokens.
> 
> Summary
> VIPRA doesn't "use" the discrete latent actions to generate the continuous outputs directly in a single step. Instead, it co-trains the model to understand both representations. The shared transformer backbone learns a rich
> representation that can be decoded into either:
> 
> 4. Discrete tokens (for latent high-level planning or compression).
> 5. Continuous vectors (for precise low-level control via flow matching).

> [!note]+ How do VLMs / LLMs usually predict stuff?
> 1. **The Output of the Transformer**
> After the data flows through all the Transformer layers (Attention, Feed-Forward, Norms), you end up with a sequence of
> vectors.
>     - What it is: A sequence of Hidden States.
>     - Shape: [Batch_Size, Sequence_Length, Hidden_Dimension]
>         - Example: [1, 100, 4096] (Batch of 1, 100 tokens long, each token is a vector of 4096 numbers).
>     - Meaning: These vectors represent the "thought" or "meaning" of each token in high-dimensional space. They are not
> probabilities yet.
> 2. **The Final Dense Layer (The "Head")**
> This is the crucial "Read-Out" layer.
>     - Mechanism: A simple Linear transformation (Matrix Multiplication).
>     - Shape: It projects from Hidden_Dimension to Vocabulary_Size.
>         - Example: Linear(4096 -> 256,000) (where 256,000 is the number of possible words/actions the model knows).
>     - Operation: Logits = Hidden_State @ Weight_Matrix + Bias
>     - Output: A vector of Logits (raw scores). Higher score = "I think this is the right token."
> 3. **Softmax (The Probability)**
> To turn those raw scores (Logits) into probabilities that sum to 100% (1.0).
>     - Operation: Probabilities = Softmax(Logits)
>     - Result: A list of probabilities for every single possible token in the vocabulary.
>         - Example: {"apple": 0.001, "move_left": 0.85, "jump": 0.14, ...}
> 4. **Selection (Taking the Action)**
> Now the model has a probability distribution. How does it choose?
> - During Training: We don't "choose." We compare the whole distribution to the correct answer (using Cross-Entropy Loss)
> to update weights.
> - During Inference (Generation): We must pick one token.
>     - Greedy Decoding: Pick the token with the highest probability (argmax). Most common for robot actions.
>     - Sampling: Randomly pick a token based on the probabilities (gives variety, but risky for robots).
> 
> Summary: The Final Pipeline
> Attention Output (4096) $\rightarrow$ `Dense Layer (4096->Vocab)` $\rightarrow$ Logits $\rightarrow$ Softmax
> $\rightarrow$ Argmax/Sample $\rightarrow$ `"Move Left"`
> 
> So, to answer your question: Yes, it directly takes the attention output and passes it through a Dense Layer. The Dense
> Layer is the bridge between "Abstract Thought" and "Concrete Vocabulary."

> [!note]+ convesation summary
> 1. Current Model Architecture (lerobot/pi05_base)
> - Structure: A "Two-Stream" architecture designed for speed (50Hz control).
>     - Brain: PaliGemma 3B (SigLIP Vision + Gemma 2B LLM). Processes Images + Text once per step.
>     - Muscle: Action Expert (Gemma 300M). A specialized 18-layer Transformer Decoder that takes the Brain's output +
> Noisy Actions.
> - Training Objective: Flow Matching (Continuous).
>     - The model predicts a "velocity vector" ($v_t$) to denoise random Gaussian noise into a smooth robot action chunk
> via ODE integration (Euler method).
> - Time Conditioning: The Action Expert uses AdaRMS (Adaptive RMS Norm) to dynamically scale its weights based on the
> diffusion timestep $t$.
> - Attention: Uses Bidirectional (Block) Attention (mask values of 0), allowing the model to "see" the entire image/text
> context and the entire action chunk simultaneously, rather than standard causal (left-to-right) masking.
> 2. Repo vs. Research Paper (The "Missing" Parts)
> - The Repo: Contains only the Continuous Control pathway of the "Post-Training" phase. It strips away the discrete
> tokenization logic to provide a clean, fast foundation model.
> - The Paper: Describes a "Pre-Training" phase where everything (including actions) is discrete tokens using a FAST
> Tokenizer. It also describes hierarchical "Subtask Prediction" (Text $\to$ Subtask $\to$ Action).
> - Missing Components: The repo lacks the FAST tokenizer, the discrete action vocabulary, and the subtask prediction
> code.
> 3. Findings on Customization (Discrete Latent Actions)
> We established that you can re-implement the paper's hierarchical "Reasoning $\to$ Action" approach using the existing
> weights:
> - The Approach: Multi-Task Learning.
>     - Branch A (Planning): Use the existing LM Head of PaliGemma (currently unused) to predict discrete tokens (Text
> plans or Quantized Action Labels).
>     - Branch B (Execution): Keep the Action Expert as-is to generate smooth continuous actions from the hidden states of
> Branch A.
> - Implementation:
>     1. Tokenizer: Add new tokens (e.g., <action_0> to <action_511>) to the tokenizer and resize the model embeddings.
>     2. Training: Add a Cross-Entropy Loss output to the VLM backbone (for the plan) alongside the existing MSE Loss for
> the Expert (for the movement).
>     3. Inference:
>         - Slow Loop (1-5Hz): Generate the "Plan" (Reasoning chain + Latent Token) using the VLM.
>         - Fast Loop (50Hz): Feed the cached Hidden State of the "Plan" into the Action Expert to generate continuous
> flow.
> 4. Key Insight: Why the "Split" Architecture?
> We compared $\pi_{0.5}$ to single-model approaches (like ViPRA/Octo):
> - Single Model: Feeds noisy actions back into the main 7B+ backbone. Con: Extremely slow inference (running 7B params x
> 10 denoising steps = ~1Hz).
> - $\pi_{0.5}$: Handing off the denoising loop to the smaller (300M) Expert allows the massive (3B) backbone to run only
> once, maintaining 50Hz real-time control without sacrificing the semantic intelligence of the large model.


Can we use non-causal attention for paligemma?

Or is this baked into the model so that we have to use the appoach that is there…??


Should we add tokens to the tokenizer and use that or should we add a linear layer to predict the action tokens??

Maybe this is an ablation… using a pure dense layer for action prediction vs adding actions to their vocab and doing hierarchical reasoning with language thinking steps
