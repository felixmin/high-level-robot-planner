---
notion-id: 2f520c92-0436-8059-8ba8-c731ae4ba33d
---
# Lerobot

HF_HOME=/mnt/data/tmp/hf HF_HUB_CACHE=/mnt/data/tmp/hf/hub HF_DATASETS_CACHE=/mnt/data/tmp/hf/datasets nohup lerobot-train   --policy.type=smolvla   --dataset.repo_id=HuggingFaceVLA/libero   --policy.load_vlm_weights=true   --policy.train_expert_only=true   --policy.num_vlm_layers=16   --dataset.use_imagenet_stats=false   --job_name=smolvla_libero_paper   --policy.n_action_steps=50   --batch_size=64   --steps=100000   --optimizer.type=adamw   --optimizer.betas="[0.9,0.95]"   --scheduler.type=cosine_decay_with_warmup   --scheduler.num_warmup_steps=100   --scheduler.num_decay_steps=99900   --scheduler.peak_lr=1e-4   --scheduler.decay_lr=2.5e-6   --save_freq=20000   --eval_freq=20000   --eval.n_episodes=10   --eval.batch_size=2   --wandb.enable=true   --policy.push_to_hub=false   --dataset.video_backend=pyav   --env.type=libero   --env.task=libero_10   --env.observation_height=512   --env.observation_width=512   --use_policy_training_preset=false &

> [!note]+ prettyprint
> HF_HOME=/mnt/data/tmp/hf HF_HUB_CACHE=/mnt/data/tmp/hf/hub HF_DATASETS_CACHE=/mnt/data/tmp/hf/datasets nohup lerobot-train   
> 
> --eval_freq=20000   
> 
> --steps=100000   
> 
> --batch_size=64   
> 
> --save_freq=20000   
> 
> --use_policy_training_preset=false
> 
> --job_name=smolvla_libero_paper
> 
>    
> 
> --policy.type=smolvla   
> 
> --policy.load_vlm_weights=true   
> 
> --policy.train_expert_only=true   
> 
> --policy.num_vlm_layers=16   
> 
> --policy.n_action_steps=50   
> 
> --policy.push_to_hub=false   
> 
> --dataset.repo_id=HuggingFaceVLA/libero  
> 
> --dataset.use_imagenet_stats=false   
> 
> --dataset.video_backend=pyav  
> 
> --eval.n_episodes=10   
> 
> --eval.batch_size=2   
> 
>  
> 
> --env.type=libero   
> 
> --env.task=libero_10   
> 
> --env.observation_height=512   
> 
> --env.observation_width=512   
> 
> --wandb.enable=true  
> 
> --optimizer.type=adamw   
> 
> --optimizer.betas="[0.9,0.95]"
> 
> --scheduler.type=cosine_decay_with_warmup   
> 
> --scheduler.num_warmup_steps=100   
> 
> --scheduler.num_decay_steps=99900   
> 
> --scheduler.peak_lr=1e-4   
> 
> --scheduler.decay_lr=2.5e-6   

## Run Smol on Libero and try to get 90% success

### Run without pretrained VLM weights

HF_HOME=/mnt/data/tmp/hf

HF_HUB_CACHE=/mnt/data/tmp/hf/hub

HF_DATASETS_CACHE=/mnt/data/tmp/hf/datasets

TRANSFORMERS_CACHE=/mnt/data/tmp/hf/transformers

lerobot-train   

--policy.type=smolvla   

--dataset.repo_id=HuggingFaceVLA/libero   

--policy.load_vlm_weights=false   

--dataset.use_imagenet_stats=false   

--job_name=smolvla_libero100   

--policy.n_action_steps=50   

--batch_size=64   

--steps=200000   

--save_freq=20000   

--eval.n_episodes=2   

--wandb.enable=true   

--policy.push_to_hub=false   

--dataset.video_backend=pyav   

--eval_freq=1000   

--eval.batch_size=2   

--env.type=libero   

--env.task=libero_10

### Run with pretrained VLM weighs

above + --policy.load_vlm_weights=true

### Run with pretrained VLM weighs

above + --policy.train_expert_only=false

# VLA Cluster

### Visualize the Long Train Run through thorter intermediate run

python scripts/submit_job.py     

submit.script=4_train_foundation     

experiment=vla_cosmos2_tokens     

model.laq.checkpoint=/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/laq-stepstep052500.ckpt     

cluster=lrz_h100 

cluster.compute.time_limit=00:20:00     

training.resume_from_checkpoint=/dss/dssmcmlfs01/pn57pi/pn57pi-dss-0001/felix_minzenmay/runs/2026-01-24_10-36-23_vla_cosmos2_tokens/checkpoints/last.ckpt     

training.max_steps=110000     

training.log_every_n_steps=1     

training.validation.check_interval=50     

training.validation.limit_batches=5     

training.validation.num_sanity_val_steps=0     

training.validation.visualization.include_freeform_pred=true
