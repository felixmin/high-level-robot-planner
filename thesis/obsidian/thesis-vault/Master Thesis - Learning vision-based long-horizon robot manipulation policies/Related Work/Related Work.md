---
notion-id: 28d20c92-0436-80a3-a840-e329cbe3efb2
---
[https://embodied-minds-lab.github.io/contact/](https://embodied-minds-lab.github.io/contact/)

> [!note]+ more
> [https://proceedings.mlr.press/v162/cohen22b/cohen22b.pdf](https://proceedings.mlr.press/v162/cohen22b/cohen22b.pdf)
> 
> A dataâ€‘driven investigation
> of human action representations
> 
> [arxiv.org](https://arxiv.org/pdf/2404.19664)
> 
> [https://arxiv.org/pdf/2203.06173](https://arxiv.org/pdf/2203.06173)
> 

> [!note]+ Cosmos World Foundation Model Platform for Physical AI
> [[Cosmos]]

> [!note]+ Behavior generation with latent actions
> [https://arxiv.org/pdf/2403.03181](https://arxiv.org/pdf/2403.03181)

> [!note]+ **Ï€0.5: a VLA with Open-World Generalization**
> [https://www.physicalintelligence.company/blog/pi05](https://www.physicalintelligence.company/blog/pi05)

> [!note]+ Emergence of Human to Robot Transfer in Vision-Language-Action Models
> [https://www.physicalintelligence.company/download/human_to_robot.pdf](https://www.physicalintelligence.company/download/human_to_robot.pdf)

> [!note]+ Unsupervised Learning for Physical Interaction through Video Prediction
> [https://proceedings.neurips.cc/paper_files/paper/2016/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2016/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf)

> [!note]+ villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models
> [https://arxiv.org/pdf/2507.23682](https://arxiv.org/pdf/2507.23682)

> [!note]+ Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations
> [https://arxiv.org/pdf/2412.14803](https://arxiv.org/pdf/2412.14803)

> [!note]+ GR00T N1: An Open Foundation Model for Generalist Humanoid Robots
> [https://arxiv.org/pdf/2503.14734](https://arxiv.org/pdf/2503.14734)

> [!note]+ CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning
> [https://arxiv.org/pdf/2505.17006](https://arxiv.org/pdf/2505.17006)

> [!note]+ CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations
> [https://arxiv.org/pdf/2505.04999](https://arxiv.org/pdf/2505.04999)

> [!note]+ mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs
> [https://arxiv.org/pdf/2512.15692](https://arxiv.org/pdf/2512.15692)
> 
> GT images vs predicted images as conditioning
> 
> 2 conditional flow matching models
> 
> discussion

> [!note]+ Behavior Generation with Latent Actions (VQBet)


 https://github.com/Physical-Intelligence/openpi 

> [!note]+ AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems
> [https://arxiv.org/pdf/2503.06669](https://arxiv.org/pdf/2503.06669)

World Models Can Leverage Human Videos for
Dexterous Manipulation

[https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/](https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/)

[https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/](https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/)

[https://arxiv.org/pdf/2402.15391](https://arxiv.org/pdf/2402.15391)

[https://arxiv.org/pdf/2511.16407](https://arxiv.org/pdf/2511.16407)

> [!note]+ R3M: A Universal Visual Representation for Robot Manipulation (2022, Chelaea Finn, Stanford, Meta, 800 cit)
> [https://arxiv.org/pdf/2203.12601](https://arxiv.org/pdf/2203.12601)
> 
> Study how visual representations learned from human video can improve downstream robotic tasks
> 
> Ego4D human video dataset
> 
> time-contrastive learning
> 
> video-language alignment
> 
> R3M as perception module for robotic learning
> 

> [!note]+ A dataâ€‘driven investigation of human action representations (2025, 


> [!note]+ Phantom: Training Robots Without Robots Using Only Human Videos (2025, Stanford, Bohg, â€¦)
> [https://arxiv.org/pdf/2503.00779](https://arxiv.org/pdf/2503.00779)

> [!note]+ Emergence of Human to Robot Transfer in Vision-Language-Action Models
> [https://www.physicalintelligence.company/download/human_to_robot.pdf](https://www.physicalintelligence.company/download/human_to_robot.pdf)
> 
> [[Emergence of Human to Robot Transfer in Vision-Language-Action Models]]

!!!! Super relevant and related survey paper !!!

> [!note]+ Towards Generalist Robot Learning from Internet Video: A Survey
> [https://arxiv.org/pdf/2404.19664](https://arxiv.org/pdf/2404.19664)
> 

## Surveys

> [!note]+ A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches (2024, unknown, China, 52 cit)
> [arxiv.org](https://arxiv.org/pdf/2404.02817)
> 
> [[A Survey of Optimization-based Task and Motion Planning From Classical To Learning Approaches]]

> [!note]+ Integrated Task and Motion Planning (2020, Lozano-Perez, MIT, 734 cit)
> [arxiv.org](https://arxiv.org/pdf/2010.01083)
> 
> 

## Latent Actions as Interface

2 base papers 

> [!note]+ IGOR: Image-Goal Representations (2024, unknown, Microsoft, China, 26 cit)
> [arxiv.org](https://arxiv.org/pdf/2411.00785)
> 
> [[IGOR]]

> [!note]+ Latent Action Pretraining from Videos (2024, Dieter Fox, Washington, Microsoft, Nvidia)
> [https://arxiv.org/pdf/2410.11758](https://arxiv.org/pdf/2410.11758)
> 
> [[LAPA Latent Action Pretraining from Videos]]

LoLA: Long Horizon Latent Action Learning for General Robot Manipulation

Learning to Act Anywhere with
Task-centric Latent Actions

Moto: Latent Motion Token as the Bridging Language for
Learning Robot Manipulation from Videos

follow ups on the base papers

> [!note]+ UniVLA: Learning to Act Anywhere with Task-centric Latent Actions (2025, unknown, U of Hong Kong, 49 cit, RSS) 
> [https://arxiv.org/pdf/2505.06111](https://arxiv.org/pdf/2505.06111)
> 
> [[UniVLA]]

> [!note]+ Latent Action Learning Requires Supervision in the Presence of Distractors
> [https://arxiv.org/pdf/2502.00379](https://arxiv.org/pdf/2502.00379)
> 
> [[Latent Action Learning Requires Supervision in the Presence of Distractors]]

> [!note]+ What Do Latent Action Models Actually Learn?
> [https://arxiv.org/pdf/2506.15691](https://arxiv.org/pdf/2506.15691)
> 
> [[What Do Latent Action Models Actually Learn]]

> [!note]+ LoLA
> [[Untitled]]

> [!note]+ LAOF
> [[LAOF]]

> [!note]+ FlowVLA


> [!note]+ LAOM


> [!note]+ LAPO


> [!note]+ Learning Latent Action World Models In The Wild


> [!note]+ 


FlowVLA: Visual Chain of Thought-based Motion
Reasoning for Vision-Language-Action Models

Latent Action Learning Requires Supervision in the Presence of Distractors

> [!note]+ From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control (2025, Abbeel, Berkeley, 18 cit)
> [https://arxiv.org/pdf/2405.04798](https://arxiv.org/pdf/2405.04798)
> 
> [[LCB]]

> [!note]+ Flow Retrieval: Flow-Guided Data Retrieval for Few-Shot Imitation Learning(2025, Dorsa Sadigh, 25 cit)
> [https://arxiv.org/pdf/2408.16944](https://arxiv.org/pdf/2408.16944)
> 
> [[FlowRetrieval]]

TODO

> [!note]+ Latent Action Diffusion for Cross Embodiment Manipulation


> [!note]+ Latent Action Learning requires Supervison in the Presence of Distractors


> [!note]+ LASER: Learning a Latent Action Space for Efficient Reinforcement Learning


> [!note]+ Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA


## Computer Vision for imroved motion detection

Look at change detection papers!! This is actually what we are doing!!

> [!note]+ ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery
> [https://arxiv.org/pdf/2511.16322](https://arxiv.org/pdf/2511.16322)

> [!note]+  https://github.com/Raessan/optical_flow_dinov3 


> [!note]+ DINO-Foresight: Looking into the Future with DINO


## Zero shot VLM high-level Planner

Language interface

> [!note]+ LLaMAR: Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments (2025, Balakrishnan, MIT, Google, etc)
> MIT, Stanford, Deepmind, â€¦
> 
> [https://arxiv.org/pdf/2407.10031](https://arxiv.org/pdf/2407.10031)
> 
> - We dont have multi-agent BUT multi agent also requires a high-level policy â†’ maybe we can learn from them
> - plan-act-correct-verify loop on high level
> - just pretrained VLM ?
> - very relevant!!!
> - states future work is using multiple smaller finetuned llms for efficiency

> [!note]+ AutoSkill: Hierarchical Open-Ended Skill Acquisition for Long-Horizon Manipulation Tasks via Language-Modulated Rewards (2025, unknown, China)
> [ieeexplore.ieee.org](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10924760)
> 
> - LLMs do very high level reasoning without grounding
> - No training of higher level policy
> - Interface is a text description
> 
> - Dynamic skill library â†’ grows over time
> - Approach
>     1. Prompt a super high level LLM about the high level goal
>     2. Split into separate skills from list or inventing new skills
>     3. For each skill a RL policy is trained without expert just the predicting rewards using our goal somehow ??

> [!note]+ Local Policies Enable Zero-shot Long-horizon Manipulation (2025, Salakhutdinov, CMU)
> [https://arxiv.org/pdf/2410.22332](https://arxiv.org/pdf/2410.22332)
> 
> - High level planner is zero shot
> 
> - Interface is 
> 
> - Main part of the paper is skill acquisition and transfer to single policy
> - III. Methods part D explains where the VLM comes in for high-level planning
> 
> - 

> [!note]+ Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (2022, Google Robotics)
> [https://arxiv.org/pdf/2204.01691](https://arxiv.org/pdf/2204.01691)
> 
> Interface is a set of skills (primitives)
> 
> High level planner is not trained (at least not majorly)
> 
> Affordance/value function that determines the probability of success is trained
> 

Bounding box interface

> [!note]+ DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping (2025, unknown, China)
> [arxiv.org](https://arxiv.org/pdf/2502.20900)
> 
> Just pretrained VLM for high level reasoning
> 
> Interface: Bounding boxes
> 
> Masking / bounding boxes â†’ how do they draw the boxes? Can the pretrained model achieve that out of the box???

Assembly graph with pose estimation / pointclouds

> [!note]+ Manual2Skill (2025, unknown, China, Toronto, Singapore, 5 cit)
> [[Manual2Skill]]
> 
> [https://arxiv.org/pdf/2502.10090](https://arxiv.org/pdf/2502.10090)
> 

> [!note]+ Blox-Net (2025, Ken Goldberg, Berkeley, 9 cit)
> [bloxnet.org](https://bloxnet.org/data/blox-net.pdf)
> 
> [https://bloxnet.org/](https://bloxnet.org/)
> 
> [[Blox-Net]]

## IL high-level Planner

Action primitives

> [!note]+ Learning to Build by Building Your Own Instructions  (2024, Dieter Fox, Washington, Nvidia)
> [[Learning to Build by Building Your Own Instructions]]

Contact point interface

> [!note]+ Hierarchical Diffusion Policy: Manipulation Trajectory Generation via Contact Guidance (2025, unknown, China, 3 cit)
> [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10912754](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10912754)
> 
> [[Hierarchical Diffusion Policy]]

Language interface web agent planners

> [!note]+ Plan and act: improving planning of agents for long-horizon tasks (2025, unknown, Berkeley / Tokyo, 43 cit)
> [arxiv.org](https://arxiv.org/pdf/2503.09572)
> 
> - This is for WEB AGENTS
>     - But it is a very similar task at its core 
> 
> - Maybe we can copy their finetuning approach
> - LLaMa-3.3-70B
> 
> > [!tip] ðŸ’¡
> > Significant performance improvement from their synthetic data
> 
> Training Pipeline for the high level planner (SFT with Synthetic data)
> 
> - Have trajectories
> - Reverse engineer plans
> - Fine tune planner and executor on synthetic dataset
> - 

> [!note]+ Learning on the Job: An Experience-Driven, Self-Evolving Agent for Long-Horizon Tasks (2025, Haifeng Li, China)
> [arxiv.org](https://arxiv.org/pdf/2510.08002)
> 
> - online learning
>     - Plan Execute Reflect Memorize
>     - Use memory to store relevant experiences
>     - Provide relevant experiences on the fly
>     - Strategic memory, procedural memory, tool memory
> â†’ Could be relevant for us aswell

## RL high-level Planner

> [!note]+ Hierarchical Visual Policy Learning for Long-Horizon Robot Manipulation in Densely Cluttered Scenes (2023, unknown, Shanghai)
> 2023
> 
> [https://arxiv.org/pdf/2312.02697](https://arxiv.org/pdf/2312.02697)
> 
> - Uses Parametrized Action Primitives (PAMDP)
>     - Each action has a low and high level component
> !![[image 1.png]]
> - Network architecture
>     - Input: Orthographic RGB-D obs
>     - One RGB one Depth encoder
>     - CLIP ResNet-50 bakcbone for feature extraction
>     - Features fusion
>     - Output heads
>         - Q-Map over all pixels â†’ argmax to select most impactful action
>         - 
> - RL for high-level planner
>     - Reward function
>         - Stepwise task progression: Reward when task progresses
>         - Spatially extended Q-update (SEQ): reward spatially spread using an anisotropic gaussian because nearby pixels usually yield similar results
>         - Two-stage update scheme (TSUS): 
> 

> [!note]+ Learn A Flexible Exploration Model for Parameterized Action Markov Decision Processes (2025, unknown, China)
> [arxiv.org](https://arxiv.org/pdf/2501.02774)
> 
> - Hybrid action models 
> - PAMDPs have
>     - bad sample efficiency
>     - perform not really well
> - Solution model-based RL for parametrized actions

> [!note]+ ARCH: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly (2025, Leonias Guibas, MIT, Stanford)
>  [arxiv.org](https://arxiv.org/pdf/2409.16451)
> 
> parameterized-action Markov decision process (PAMDP)

## Other

> [!note]+ Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation (2023, Karen Liu, FeiFei Li, Stanford, USES LEGO)
> Inkl FeiFei Li
> 
> [https://arxiv.org/pdf/2309.00987](https://arxiv.org/pdf/2309.00987)
> 
> [GitHub - sequential-dexterity/SeqDex: "Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation" code repository](https://github.com/sequential-dexterity/SeqDex/tree/master)
> 
> > [!tip] ðŸ’¡
> > I think skill order is predefined
> 
> - Most works study one dextrous skill
> - Chaining multiple such skills is underexplored
> - End state distribution of prior skill has to match with next skill
> 
> > [!tip] ðŸ’¡
> > 
> > - They have LEGO construction in ISAAC sim 
> 
> > [!note]+ Top down camera block identification
> > !![[image 2.png]]
> 
> > [!note]+ Wrist camera block segmentation
> > !![[image 3.png]]
> 
> > [!note]+ Sim
> > !![[image 4.png]]

> [!note]+ Relay Policy Learning:Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (2019, Sergey Levine, Corey Lynch etc, BAIR)
> [Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning](https://relay-policy-learning.github.io/)
> 
> [https://arxiv.org/pdf/1910.11956](https://arxiv.org/pdf/1910.11956)
> 
> Starts with IL and then continues with RL?

> [!note]+ DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous  Manipulation (2025, unknown, China)
> [arxiv.org](https://arxiv.org/pdf/2509.23829)

## Segmentation

> [!note]+ LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning (2024, unknown, ETH)
> [https://arxiv.org/pdf/2404.08767v1](https://arxiv.org/pdf/2404.08767v1)

## Todo

> [!note]+ The Art of Imitation: Learning Long-Horizon Manipulation Tasks from Few Demonstrations (2024, unknown, Freiburg)
> [arxiv.org](https://arxiv.org/pdf/2407.13432)

> [!note]+ TidyBot: Personalized Robot Assistance with Large Language Models (2023, Funkhouser, Bohg, etc, Stanford, Google, etc)
> [https://arxiv.org/pdf/2305.05658](https://arxiv.org/pdf/2305.05658)
> 

> [!note]+ DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability (2024, Dieter Fox, MIT, Nvidia, 26 cit)
> [https://arxiv.org/abs/2306.13196](https://arxiv.org/abs/2306.13196)

> [!note]+ Generative Skill chaining: Long-Horizon Skill Planning with Diffusion Models (2023, Danfei Xu, Georgia Tech, 106 cit)
> [https://arxiv.org/pdf/2401.03360](https://arxiv.org/pdf/2401.03360)

[https://arxiv.org/html/2403.14536v1](https://arxiv.org/html/2403.14536v1)

[https://www.ri.cmu.edu/publications/high-level-planning-and-low-level-execution-towards-a-complete-robotic-agent/](https://www.ri.cmu.edu/publications/high-level-planning-and-low-level-execution-towards-a-complete-robotic-agent/)

[https://academicweb.nd.edu/~lemmon/projects/NSF-12-520/pubs/2014/HierarchicalControl.pdf](https://academicweb.nd.edu/~lemmon/projects/NSF-12-520/pubs/2014/HierarchicalControl.pdf)

[https://arxiv.org/pdf/2011.00642](https://arxiv.org/pdf/2011.00642)

[https://arxiv.org/abs/2401.06833](https://arxiv.org/abs/2401.06833)

[staff.ustc.edu.cn](http://staff.ustc.edu.cn/~zkan/Papers/Conference/%5B44%5D_2023ICRA.pdf)

[https://arxiv.org/pdf/2407.09792](https://arxiv.org/pdf/2407.09792)

[https://arxiv.org/pdf/2410.22332](https://arxiv.org/pdf/2410.22332)

[https://arxiv.org/pdf/2502.04531v1](https://arxiv.org/pdf/2502.04531v1)

[https://openaccess.thecvf.com/content/CVPR2023/papers/Decatur_3D_Highlighter_Localizing_Regions_on_3D_Shapes_via_Text_Descriptions_CVPR_2023_paper.pdf](https://openaccess.thecvf.com/content/CVPR2023/papers/Decatur_3D_Highlighter_Localizing_Regions_on_3D_Shapes_via_Text_Descriptions_CVPR_2023_paper.pdf)

[https://openreview.net/pdf?id=4CLiGBQV3U](https://openreview.net/pdf?id=4CLiGBQV3U)

[https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TopNet_Transformer-Based_Object_Placement_Network_for_Image_Compositing_CVPR_2023_paper.pdf](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TopNet_Transformer-Based_Object_Placement_Network_for_Image_Compositing_CVPR_2023_paper.pdf)

[https://openaccess.thecvf.com/content/CVPR2025/papers/Parihar_MonoPlace3D_Learning_3D-Aware_Object_Placement_for_3D_Monocular_Detection_CVPR_2025_paper.pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Parihar_MonoPlace3D_Learning_3D-Aware_Object_Placement_for_3D_Monocular_Detection_CVPR_2025_paper.pdf)

[https://arxiv.org/pdf/2505.05288v2](https://arxiv.org/pdf/2505.05288v2)

[https://arxiv.org/pdf/2403.17246](https://arxiv.org/pdf/2403.17246)

[https://arxiv.org/pdf/2410.02189v1](https://arxiv.org/pdf/2410.02189v1)

[https://arxiv.org/pdf/2409.05864](https://arxiv.org/pdf/2409.05864)

[https://arxiv.org/pdf/2503.22634](https://arxiv.org/pdf/2503.22634)

[https://arxiv.org/html/2506.14608v3](https://arxiv.org/html/2506.14608v3)

[https://openreview.net/pdf/107f4c0a745d62a2ad87acd28cf763c88c49014f.pdf](https://openreview.net/pdf/107f4c0a745d62a2ad87acd28cf763c88c49014f.pdf)

[https://arxiv.org/html/2509.26251v1](https://arxiv.org/html/2509.26251v1)


Good approach: Reverse engineer to the surveys

1. Find an interesting paper
2. Look at where it is cited in current surveys
3. Find its impact and what it lacks

[[Untitled]]
