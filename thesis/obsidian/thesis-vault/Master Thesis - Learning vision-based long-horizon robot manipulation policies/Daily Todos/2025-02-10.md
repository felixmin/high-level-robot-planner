---
notion-id: 30320c92-0436-80ff-bfbf-fc763b745ea0
base: "[[Daily Todos.base]]"
---
## Finetuning Cosmos

- run the dataloader through oxe and run sample counters to see how much data we have of each actually
- run cosmos reason 2 finetune again
- run cosmos predict finetune
> [!note]+ questions
> - whats the maximum training steps that you can do while finetuning without destroying the network? how many steps are the actual training?
> → we cannot cotrain to preserve knowledge because we dont have the original recipe / data
> - is there an official finetuner mixes your data in the original distribution and thereby maintains knowledge?
> - [https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason2/intelligent-transportation/post_training.html](https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason2/intelligent-transportation/post_training.html)

## Dataloader

- Do a full pass through the dataset with different configs and locally / on cluster to see the throughput and how much data we actually have available
- Reach out to lrz because some datasets are broken

## Meeting Jonas Pai

Notes on mimic-video

- They argue Video models have better pretraining → what about abstract reasoning about the env?
- Will the video model be the solution or a VLA high level + video model for low level

Questions

- horizon
    - How is the performance in long horizon tasks? long horizon video generation is still hard ?? 
    - Impact of video horizon on action decoder performance

- architecture
    - How did they go from future GT frame to “latent action” in the oracle case?
    - Did they use interweaving or just “stitch on” their head?
        - Their action decoder inputs… just a latent vector?
        - Action decoder under different conditioning strategies
            - interweaving, noise level, prediction horizon
    - Action decoder is just out of the box diffusion model?
    - is a video backbone a good planner or a good dynamics model?

- training
    - How did they finetune cosmos predcit?
        - LoRA…
        - Which dataset?
        - How many steps?
        - How many episodes / samples?
    - 


just layer 19

- first or last was bad

if you denoise too much it becomes bad 

if the video 

70k on bridge

5k on libero

how to finetune the video model

on policy rl with vlm judge

rl is razor…

→ on policy 

video model requirements

- long texturaldescriptions
- short in dataset
- → in eval short → thats why short

- you have to include data you multiply the first and last frames

→ otherwise it doesnt have a sense of when somehting is done

→ when task is done —> do nothing

1x had the same thing

biggest painpoints

→ video dataloading

→ fast video dataloading is a bottleneck

→ batch size per gpu 1 → 

1million h200 hours

other ablations

- decoupling the flow matching made it work
- noise distribution varying didtnt change performance
- horizons were not ablated

flow matching params didnt make a big difference

layer was unfortunately very important

train on low noise and then go to low noise and it works better

they just used the action decoder from pi

## Other

- fix nvidia-smi on workstation
- setup codex memory
- setup tmux stats viewer with local and remote