---
notion-id: 29420c92-0436-80ac-9bbf-d91db7789e11
---
Designing a high-level planner involves choosing both **how to generate plans** and **how to represent the plan or goals** for the low-level policy. Recent literature spans a spectrum from using *pre-trained vision-language models (VLMs)* and *large language models (LLMs)* for planning, to traditional *hierarchical RL* and imitation learning. A core design question is the **interface** between the high-level planner and the low-level control policy – whether plans are communicated in natural language, images, latent embeddings, or precise coordinates. Below we survey existing approaches, compare goal representation modalities, discuss recent trends, and recommend a direction for a LEGO assembly scenario.

## Pre-Trained VLM/LLM-Based Planners

**Vision-Language Models for Planning:** A popular recent approach is to leverage large pre-trained models (like GPT-4, PaLM, or CLIP-like vision-language models) to generate high-level plans. These models come with broad world knowledge and reasoning ability, enabling *open-vocabulary instructions* and generalization to new tasks. For example, Google’s **PaLM-SayCan** uses an LLM (PaLM) to break a high-level instruction into feasible steps by scoring possible skill actions; combined with a value function for grounding, this enabled a robot to complete 101 diverse household tasks with a **plan success rate ~84%** (execution 74%)[say-can.github.io](https://say-can.github.io/#:~:text=performance%20of%20the%20entire%20system,Initial%20release%20of%20SayCan)[say-can.github.io](https://say-can.github.io/#:~:text=The%20proposed%20approach%20achieves%20an,enhancing%20the%20underlying%20language%20model). Crucially, the LLM was not re-trained from scratch – it was used largely as-is (with prompt engineering), highlighting that *pre-trained models can be applied zero-shot or with minimal fine-tuning*.

**Hierarchical Vision-Language Planning:** Another example is a recent 3-layer hierarchy for a humanoid robot: a low-level RL controller, mid-level learned motor skills, and a high-level VLM-based planner[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=system%20comprises%20three%20layers%3A%20,These%20experiments)[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=trained%20via%20imitation%20learning%20that,step%20manipulation). The high-level module takes a **natural-language goal and visual observation** as input and uses a pre-trained VLM to sequence skill policies accordingly[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=match%20at%20L360%20Formally%2C%20a,the%20conditions%20described%20by). In real-world trials, this VLM-driven planner achieved **72.5% success** on multi-step manipulation tasks[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=trained%20via%20imitation%20learning%20that,step%20manipulation) – demonstrating that *even without fine-tuning the VLM, using it to guide skill selection yields robust long-horizon performance*. Many recent systems follow this pattern: they prompt a VLM/LLM with the task description and current state (often via images or text) and let it output a sequence of sub-tasks in natural language or a symbolic form. The low-level then executes those sub-tasks with its learned skills. This approach excels in generality (thanks to the language model’s knowledge) and requires far less task-specific training data than end-to-end RL.

**Structured Plan Representations with LLMs:** Some works integrate LLMs by having them produce *structured commands or code* as the plan. For instance, an LLM can output a sequence like `"Pick(red_brick); Attach(red_brick, blue_brick)"` or even generate pseudo-code that calls robot API functions. This retains the semantic richness of language while providing a formal interface to the robot. In general, LLM-based planning is attractive because it offloads cognitive reasoning to the model (which has seen vast data), leaving the robot to focus on grounding and execution. The downside is that language models may output infeasible or unsafe steps if not properly constrained – hence many systems use additional checks or value estimates (as in SayCan) to *ground the LLM’s suggestions in reality*[say-can.github.io](https://say-can.github.io/#:~:text=The%20main%20principle%20that%20we,to%20weight%20the%20skill%27s%20likelihood)[say-can.github.io](https://say-can.github.io/#:~:text=In%20summary%2C%20given%20a%20high,output%20step%20is%20to%20terminate).

## Reinforcement Learning for High-Level Planning

**Hierarchical RL:** Before the surge of foundation models, a common approach was **hierarchical reinforcement learning (HRL)** – training a high-level policy (via RL) to choose subgoals or skills, with a low-level policy executing those choices. In principle, HRL can learn an optimal decomposition of tasks. In practice, however, pure RL at the high level is challenging for long-horizon tasks like assembly: it suffers from sparse rewards (e.g. only getting feedback after a full assembly is complete), huge search spaces, and sample inefficiency. Studies have shown that as the number of skills or subtask options grows, learning a high-level policy becomes difficult without additional guidance[eehpc.ece.jhu.edu](https://eehpc.ece.jhu.edu/wp-content/uploads/2023/10/LangRob_CoRL.pdf#:~:text=tackle%20these%20problems%20include%20using,increases%2C%20we%20face%20some%20of)[eehpc.ece.jhu.edu](https://eehpc.ece.jhu.edu/wp-content/uploads/2023/10/LangRob_CoRL.pdf#:~:text=Hierarchical%20Reinforcement%20Learning%20,situations%20is%20that%20we%20almost). Modern approaches therefore often **augment or replace high-level RL with heuristics or pre-trained models**. For example, one strategy is to use an LLM to suggest subgoals and use RL only to refine or execute them, which was found to significantly improve sample efficiency and performance[eehpc.ece.jhu.edu](https://eehpc.ece.jhu.edu/wp-content/uploads/2023/10/LangRob_CoRL.pdf#:~:text=exploit%20the%20planning%20capabilities%20of,once%20trained%2C%20don%E2%80%99t%20need%20access). In summary, while RL *can* be used to learn a planner (and some research does compare various goal representations under RL), it tends to be less favored for complex open-ended tasks today due to training hurdles. Instead, RL is commonly used at the **low-level control** (e.g. a locomotion or grasping policy), with higher-level decisions made by other means[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=system%20comprises%20three%20layers%3A%20,These%20experiments).

**Evolutionary or Search-Based Planning:** Another approach (not learning-based) is classical task planning or evolutionary search. For tasks like LEGO assembly, one can use brute-force or informed search in the space of assembly sequences. Recent work on robotic LEGO assembly used **assembly-by-disassembly search** with physics simulation to find a valid sequence of brick placements[dyalab.mines.edu](https://dyalab.mines.edu/2025/icra-workshop/13.pdf#:~:text=III.%20PHYSICS,ASP%20for%20combinatorial%20assembly%20is)[dyalab.mines.edu](https://dyalab.mines.edu/2025/icra-workshop/13.pdf#:~:text=address%20the%20challenge%2C%20we%20integrate,structure%2C%20the%20method%20generates%20a). Such planners output an explicit sequence of actions (e.g. which piece to attach next and where) and guarantee feasibility (stability) of each step. However, they rely on known models of the goal structure and are computationally heavy for large assemblies. Evolutionary algorithms could also optimize a plan (sequence) for some objective (e.g. stability or speed), but these are typically domain-specific solutions. In general, *search or programmatic planners can produce very precise, structured plans (often as a list of discrete actions or poses), but lack the flexibility and learning capabilities of modern ML-based planners*. They also assume the goal is fully specified (e.g. a complete 3D model of the target in assembly problems).

## Imitation Learning & Fine-Tuned Planners

**Learning from Demonstrations:** Instead of using a frozen pre-trained model or training via RL, one can train a high-level planner via **behavioral cloning or supervised learning** on example plans. For instance, if we have many demonstrations of tasks (either from humans or solved by other means), we can train a policy to map from the current state and goal to the next high-level action. This is effectively *imitation learning* for planning. Recent work often combines this with the modalities above: e.g. training a network that takes in an image of the scene and a language instruction and outputs the next action or subgoal. One notable example is **GRIF (Goal Representations for Instruction Following)**, which trains a policy that can be conditioned on *either* a language instruction or a goal image[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Image). By joint training with both modalities, the policy learns a shared latent task representation that aligns language and vision. This yields a planner that benefits from unlabeled robot experience (using goal images for training) while still understanding human language commands[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Conditioning%20on%20visual%20goals%20%28i,pixel%20with%20other%20states)[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=is%20to%20provide%20a%20goal,digesting%20large%20unstructured%20robot%20datasets). In general, fine-tuning or training your own high-level model with imitation can achieve strong performance *if you have sufficient task-specific data*. It has been used in fixed domains (like a kitchen environment with fixed goals), but obtaining a large diverse dataset of plans for open-ended tasks is challenging. Thus, many projects bootstrap with human-written instructions or simulations to train such models, and often they still leverage pre-trained encoders (for images or text) as a starting point.

**Combination with Online Learning:** Some approaches also allow the high-level planner to continue learning online (e.g. fine-tune on new successes/failures or using evolutionary strategies to refine plans). For example, one could use evolutionary search to mutate and select high-level plans, or fine-tune a planning policy with reinforcement signals if a subplan fails. These hybrid methods are still in early stages. Overall, *imitation learning gives a good initial policy, and online learning can adapt it*, but the most common trend is to rely on the generalization of large pre-trained models as much as possible before resorting to expensive online training.

## Goal Representation Modalities (Interface to Low-Level)

A crucial design choice is **how the high-level planner communicates the goal or subtask to the low-level policy**. Several modalities are used in the literature, each with pros and cons:

- **Natural Language Goals:** High-level plans can be expressed as text (e.g. *“pick up the red 2x2 brick and attach it to the base”*). This is very human-interpretable and leverages the expressivity of language to specify virtually any goal. Many VLM-based planners naturally output text instructions as their plan[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=match%20at%20L360%20Formally%2C%20a,the%20conditions%20described%20by). The advantage is **open-vocabulary generalization** – new tasks can be described that were never explicitly seen in training – and easy integration of human input (a human can read or even edit the plan). However, the **downside of language** is grounding and precision: the low-level controller must translate the instruction into concrete motor actions, which requires understanding the semantics (e.g. what is the “red 2x2 brick” and where is “the base”). Training a low-level *language-conditioned policy* is difficult unless you have lots of language-labeled demonstrations. Pure language can also be ambiguous or omit spatial details needed for precise control. In short, language is *powerful for abstract specification but harder to ground to actual motions*[guydavidson.me](https://guydavidson.me/files/position_human_like_rl_structured_expressive_goals.pdf#:~:text=Figure%201,but%20are%20substantially%20harder%20to). It’s great as a top-level interface (especially for humans), but many systems pair it with additional modalities for grounding.
- **Visual Goals (Images):** Another common interface is to use an **image of the goal state or subgoal**. For example, the high-level planner might provide an image showing what the assembly should look like after the next step. Visual goal representations are very concrete – the low-level policy can directly compare its camera input to the goal image. Indeed, conditioning policies on goal images has proven highly effective in robotics because you can generate unlimited training goals via *hindsight replay* (any achieved state can be turned into a goal)[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Conditioning%20on%20visual%20goals%20%28i,pixel%20with%20other%20states). This approach, known as *goal-conditioned policies*, sidesteps the need for language annotation and provides dense grounding (pixel-wise or feature-wise comparison tells you how close you are to the goal). **The benefit:** visual similarity is a natural metric and doesn’t require the robot to “understand” language. For instance, a robot arm can be trained to reach any arrangement of objects given a picture of that arrangement as goal – this is easier for the policy to learn than following a text instruction directly[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Conditioning%20on%20visual%20goals%20%28i,pixel%20with%20other%20states). In LEGO assembly, a goal image could be the diagram from the instruction manual for the next step, giving the robot an exact target configuration to achieve. **However,** images have downsides too: they are not *intuitive for humans to provide* (a human user would have to manually create or find an image of the desired outcome, which is often as hard as doing the task)[arxiv.org](https://arxiv.org/abs/2204.11134#:~:text=low,humans%20to%20specify%20and%20use)[arxiv.org](https://arxiv.org/abs/2204.11134#:~:text=interpret%20for%20non,shot%20goal%20specification%2C%20and). Images also need to be from the right perspective and lighting to be comparable to the robot’s view – a mismatch can confuse the low-level. Despite these issues for *user input*, internally many systems use images or rendered scenes as an efficient way to pass goals. Notably, recent research even explored *planning purely in the visual domain* (“visual planning”) and found that for highly spatial problems (like navigation mazes), a sequence of images can outperform a sequence of text steps[arxiv.org](https://arxiv.org/html/2505.11409v1#:~:text=We%20validate%20the%20feasibility%20of,in%20the%20visual%20planning%20paradigm). This suggests that for tasks requiring geometric reasoning (like physical assembly), visual representations are extremely valuable.
- **Latent Embedding Goals:** Rather than explicit language or pixels, some approaches use a **learned latent representation** as the interface. Here, both the high-level planner and low-level policy are trained (or designed) to communicate via a vector in some embedding space. For example, a VAE or contrastive model might encode states and goals into a latent $z$; the high-level outputs a target $z$, and the low-level is trained to reach states with that latent code. The idea is to get a compact and smooth representation of goals that the low-level can follow. Latent goals can be efficient and might capture abstract features (e.g. “assembledness” of a structure) that aren’t obvious in raw sensor space. Moreover, if the latent space is *aligned across modalities*, the high-level could take in language or images and output a latent that represents the same concept to the robot. In fact, the GRIF approach above does exactly this: it learns a shared latent space for image-goals and language instructions, and uses it to condition the policy[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Image)[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=trains%20a%20language,on%20mostly%20unlabeled%20demonstration%20data). The challenge with latent representations is that they are **not human-interpretable** – which makes debugging or human-in-the-loop adjustment harder. They also require an extensive training procedure to ensure the latent truly captures goal semantics. Nonetheless, research (e.g. **LIV: Language-Image Value Learning**) has shown it’s possible to train multi-modal embeddings that encode *“goal achievement”* in a single representation[penn-pal-lab.github.io](https://penn-pal-lab.github.io/LIV/#:~:text=How%20can%20we%20pre,robotics%20manipulation%20and%20reward%20specification)[penn-pal-lab.github.io](https://penn-pal-lab.github.io/LIV/#:~:text=learning%20and%20mutual%20information%20contrastive,language). Such embeddings can even be used to assign reward or success metrics across image and language goals in a unified way, and have improved policy learning in multi-step tasks[penn-pal-lab.github.io](https://penn-pal-lab.github.io/LIV/#:~:text=encodes%20a%20goal,world%20robot%20environments%20that)[penn-pal-lab.github.io](https://penn-pal-lab.github.io/LIV/#:~:text=LIV%20as%20Representation%20for%20Language,BC). In summary, latent goal representations are *powerful under the hood* and often accompany modern architectures (e.g. using CLIP or contrastive learning to align language, image, and state), but they are typically wrapped by more interpretable modalities at the user level.
- **Explicit Structured Goals (Coordinates/Keypoints):** In very structured domains, the high-level planner might output low-dimensional explicit parameters – for instance, the 3D coordinates where the next brick should be placed, or the identity of an object and an exact pose. This is common in classical robotics and motion planning: a task planner might say “place part A at (x,y,z) orientation (α,β,γ)”. The low-level then just executes a motion to that pose (via control or inverse kinematics). The obvious advantage is **precision** and clarity – there is no ambiguity in what needs to be done if the coordinates or contact points are specified. For tasks like LEGO assembly, one could imagine the high-level computing the exact stud connections for the next brick (perhaps by reading a CAD model or instruction diagram) and telling the low-level “attach brick at these stud positions”. Some recent assembly planners indeed reason about *contact points and stability* – e.g. a planner that ensures each brick placement will be physically stable, outputting a sequence of stable placements[dyalab.mines.edu](https://dyalab.mines.edu/2025/icra-workshop/13.pdf#:~:text=comprehensive%20understanding%20of%20the%20underlying,planning%20framework%20for%20safe%2C%20efficient)[dyalab.mines.edu](https://dyalab.mines.edu/2025/icra-workshop/13.pdf#:~:text=III.%20PHYSICS,ASP%20for%20combinatorial%20assembly%20is). However, *hard-coded coordinate interfaces lack generality*: they require the environment to be calibrated and the objects well-modeled. They also don't scale to more abstract tasks (you can’t easily specify “tidy the room” with a fixed set of coordinates). Additionally, a purely geometric specification might omit higher-level context (the robot might place the brick exactly at the coordinate even if the orientation is wrong or an obstacle is in the way, unless everything is accounted for). Thus, while explicit structured goals are **useful in narrow domains** (and might be part of the solution for assembly), they are usually combined with other representations for flexibility. For instance, a high-level might first use language or images to identify *which* part to place (open-ended), then use a known geometric model to compute *where* to put it precisely – handing those coordinates to low-level.

**Comparative Success:** In terms of *empirical success*, **goal images and states** have historically led to higher success rates in policy learning than raw language specifications, due to ease of grounding[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Conditioning%20on%20visual%20goals%20%28i,pixel%20with%20other%20states). Many goal-conditioned RL studies report that an agent can reach a goal configuration given an image with high reliability if trained on sufficient data (thanks to techniques like hindsight relabeling). On the other hand, **language** has become increasingly successful as we learn to integrate it: modern vision-language planners can achieve complex tasks by leveraging semantic knowledge (e.g., SayCan’s robot completing 8-step long-horizon instructions that would be infeasible via pure RL)[say-can.github.io](https://say-can.github.io/#:~:text=Image)[say-can.github.io](https://say-can.github.io/#:~:text=In%20the%20next%20example%2C%20SayCan,the%20necessity%20of%20affordance%20grounding). The trend is to **combine modalities** to harness their complementary strengths. For example, one can use language at a top level to specify *what* to do in general, and use vision at a lower level to specify *how it should look* when done[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=is%20to%20provide%20a%20goal,digesting%20large%20unstructured%20robot%20datasets). Indeed, recent research explicitly aligning language and image goal representations shows significantly improved generalization and performance[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=trains%20a%20language,on%20mostly%20unlabeled%20demonstration%20data)[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=task%20representation%20from%20the%20corresponding,policy%20network%20to%20predict%20actions). A clear pattern is emerging: *robots that understand both language and vision can follow human instructions more flexibly while still achieving the precise outcomes that visual goal conditioning provides*.

## Recent Trends and Shifts

Over the past few years, there has been a **notable shift in high-level planning for robots**:

- **From Task-Specific to Generalist:** Earlier work often built task-specific planners (or skill graphs) for a fixed set of operations (closed vocabulary). Now there’s a push for *generalist robots* that can handle open-ended tasks specified in natural language. This is fueled by the advent of foundation models. As a result, **open vocabulary interfaces (language, open-set images)** are getting far more attention than fixed symbolic planners.
- **Foundation Models in the Loop:** Whereas planning used to rely on hand-crafted domain knowledge or training from scratch, it is now common to drop in a pre-trained model. For example, LLMs are used as a source of “common sense” and reasoning, effectively providing the high-level policy with priors about what sequences of actions make sense[say-can.github.io](https://say-can.github.io/#:~:text=The%20main%20principle%20that%20we,to%20weight%20the%20skill%27s%20likelihood)[say-can.github.io](https://say-can.github.io/#:~:text=Once%20the%20skill%20is%20selected%2C,Done). Vision-language models like CLIP are used to define reward functions or goal match functions (e.g., computing similarity between the current scene and a described goal)[arxiv.org](https://arxiv.org/abs/2204.11134#:~:text=low,humans%20to%20specify%20and%20use)[arxiv.org](https://arxiv.org/abs/2204.11134#:~:text=goal%20specification%20that%20are%20expected,world%20datasets). The community has realized that these models, trained on internet-scale data, **encode a wealth of relevant knowledge** that can be harnessed for planning without massive robot-specific datasets.
- **Less Fine-Tuning of Language Models:** Many approaches deliberately *avoid fine-tuning the largest models*, because it’s expensive and risks losing their generality. Instead, techniques like prompt engineering, few-shot prompting, or lightweight adapters are used. The high-level planner is often composed of a frozen LLM for reasoning plus a learned module for grounding (e.g., a value function or a smaller policy that interprets the LLM’s output). This means research papers often demonstrate new planning capabilities by *cleverly prompting GPT-4 or PaLM*, rather than training a new transformer from scratch. This trend lowers the barrier to integrating new modalities – e.g. one can prompt an image-captioning model to output a plan given a scene image and instruction.
- **Multimodal Goal Specification:** There is growing interest in allowing **“any modality” for task specification**. A user might show a picture of a desired arrangement, give a verbal instruction, or even sketch a goal, and the robot should handle it. A 2022 study explicitly explored using internet images, hand-drawn sketches, or simple phrases as goal definitions for manipulation, finding *promising zero-shot results* by using pre-trained models to interpret those goals[arxiv.org](https://arxiv.org/abs/2204.11134#:~:text=studied%20approach%20to%20task%20specification,As%20a)[arxiv.org](https://arxiv.org/abs/2204.11134#:~:text=preliminary%20step%20towards%20this%2C%20we,world%20datasets). This indicates a shift towards *goal modality agnosticism* – the robot should align whatever input the human finds easiest into its internal goal representation.
- **Bridging High-Level and Low-Level:** While high-level planning has embraced language and vision, we see a complementary push to make low-level controllers more *goal-conditioned and flexible*. Instead of training a separate low-level for each skill, researchers train general policies that take a goal description. For instance, a single policy might handle “place object A on B” for any A, B given appropriate goal context. This synergy means the interface can be richer (not just selecting one of N discrete skills, but providing a continuous goal to a universal skill policy). Such architectures blur the line between high and low level but are powerful for generalization.

In summary, **early 2020s approaches favored learning with images/states or manually defined subgoals**, whereas **late 2020s approaches increasingly leverage language and multimodal representations** to achieve more generality. There is a clear rise in approaches that use *pre-trained VLMs for planning and supervision*, reflecting the broader trend of merging robotics with AI foundation models.

## Recommendation: High-Level Plan Representation for LEGO Assembly

Given this context, for a **LEGO assembly** high-level planner, we suggest a **hybrid vision-language strategy** for the interface. LEGO assembly is inherently a spatial, visual task – the goal is to build a structure that matches a target design. At the same time, it is helpful to incorporate the *existing assembly instructions* (which often include diagrams and sometimes textual hints) and allow human oversight. A combination of representations can achieve precision, generality, and interpretability:

- **Leverage Vision for Geometric Precision:** It makes sense for the high-level planner to use **images or visual observations to specify subgoals** for the low-level policy. For instance, the planner can provide a *goal image of the partially built model after the next step*. Since you mentioned having the assembly instructions, the planner could pull the diagram of the *next assembly step* and use that as the goal state. The low-level policy (with an RGB camera on the robot) would then attempt to make the current assembly match that image. This approach taps into the strength of goal-image conditioning (easy grounding and dense feedback)[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Conditioning%20on%20visual%20goals%20%28i,pixel%20with%20other%20states). It would likely require training the low-level with image goals (perhaps in simulation or via real demos with hindsight relabeling), but once trained, such a policy can robustly execute each assembly step by “matching” the target image.
- **Incorporate Language for Flexibility and Teaching:** In parallel, using **language as part of the interface** will provide the needed generalization and human-in-the-loop capability. The high-level planner can output a brief *textual description of the action* like “Attach the red 2x2 brick on top of the current structure, aligning it as shown.” This natural language plan step can be shown to a human supervisor or used for logging and debugging. More importantly, by maintaining a language-friendly representation, the system can generalize to other tasks (e.g. in a household scenario: “open the cabinet and put the item in”) without needing a pre-defined image of the goal for everything. In LEGO assembly, language helps specify *which piece* to use and *where*, in human terms. If the low-level policy is not language-driven, the system can still use the text internally – for example, to query a knowledge base (“what is a red 2x2 brick?”) or to interface with a human (“Should I proceed to attach the roof piece?”). Having a language layer aligns with the trend of open-vocabulary and makes the planner *more understandable and editable* by developers or users[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=is%20to%20provide%20a%20goal,digesting%20large%20unstructured%20robot%20datasets).
- **Structured Action Parameters:** We also recommend outputting a **structured parameterization** of the action alongside the above (this could be derived from vision or CAD data). For instance, the high-level plan for a step could include the exact **part ID and pose** where it should be placed. Concretely, the planner (with access to the LEGO model specification) could determine that “Brick #5 goes at coordinates (x,y,z) relative to Brick #2.” This is akin to the contact-point representation: it removes ambiguity for the low-level. The low-level controller in this case could be a motion planner or learned policy that, given a target part and pose, actually moves the robot arms to assemble it. While you *could* try to have the low-level infer this from the image alone, providing the coordinates or keypoints explicitly would likely improve reliability. Essentially, the hierarchy might work as: **LLM/VLM interprets instructions -> identifies next piece and target location (language + structured data) -> low-level executes placement**. This mirrors how human builders use manuals: they identify the piece by name/ID, look at the diagram to see where it attaches, then use hand-eye coordination to put it there.
- **Human-in-the-Loop Adjustments:** With a vision-language plan, a human can easily intervene. If the plan says “attach red brick on top” and the human notices a mistake, they can correct that in language (“actually, use the *2x4* brick instead”) and the system, ideally, could re-generate or adjust the plan. Because the plan is not a black-box latent but an intelligible sequence of steps (with images and text), it’s much easier to adjust mid-course. This aligns with your desire to have a human able to adjust the building process.

**Why this multi-modal approach?** It capitalizes on the success seen in recent work by combining modalities: language for abstract task understanding and *infinite task variety*, and vision for concrete goal grounding[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=is%20to%20provide%20a%20goal,digesting%20large%20unstructured%20robot%20datasets). In fact, research explicitly combining language instructions with aligned visual goals has demonstrated improved generalization across diverse scenes[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=trains%20a%20language,on%20mostly%20unlabeled%20demonstration%20data)[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=task%20representation%20from%20the%20corresponding,policy%20network%20to%20predict%20actions). For LEGO assembly, the high-level could even use a *pre-trained VLM* (like PaLM-E or GPT-4 with vision) to read the human-provided assembly instructions (which might be a PDF or images), interpret them, and output the sequence of steps in a robot-understandable form. This avoids having to manually encode the assembly knowledge. Each step, represented with an image snippet of the instruction and a text description, is then fed to low-level control.

In practice, you might start with a simpler version: use the given LEGO instruction steps directly as the plan (since those are essentially an expert-crafted plan). The high-level planner in that case just needs to index the current step and maybe verify completion (this is similar to the VLM-based *execution monitor* in the humanoid example, which used a VLM to check if the last subgoal was achieved from vision[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=manipulation%20tasks,completion%2C%20orchestrating%20transitions%20between%20skills)[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=completion%2C%20orchestrating%20transitions%20between%20skills)). Over time, you can enhance it to handle deviations (if a piece is misplaced, the vision model detects it and could re-plan or ask for human help).

**Which representation to choose?** If we must choose a single representation for the interface, consider the capabilities of your low-level policy. Since you plan to use an RL or IL-based low-level, a **goal image** or **visual state** target is often the most straightforward for it to handle (many low-level policies are trained to minimize distance to a goal state). Therefore, providing an image of the desired outcome for the next subtask is likely to be successful. Yet, to retain open-ended generalization, wrapping that inside a **language-conditioned framework** is wise – essentially the approach of *language-directed, vision-grounded planning*. This way, your planner can easily pivot to other use cases (household tasks) by accepting language goals, and internally either produce a series of images (if available, like diagrams) or invoke learned skills. The literature suggests this combined approach is not only feasible but increasingly common, as it marries the intuitiveness of language with the concreteness of visual goals[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Conditioning%20on%20visual%20goals%20%28i,pixel%20with%20other%20states)[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=is%20to%20provide%20a%20goal,digesting%20large%20unstructured%20robot%20datasets).

In conclusion, **a multi-modal high-level planner** that communicates *“what to do” in language and *“how it should look”* in vision (or precise coordinates) would be most suitable. It aligns with the state of the art – many recent papers demonstrate that using *pre-trained models for language/vision planning, without relying purely on RL, yields robust long-horizon performance*[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=trained%20via%20imitation%20learning%20that,step%20manipulation)[say-can.github.io](https://say-can.github.io/#:~:text=Image). Such a planner will be extensible beyond LEGO, and in the near term, it can directly utilize your LEGO assembly instructions to guide the low-level policy step-by-step. By choosing this representation, you tap into current best practices and ensure your system benefits from both human-friendly communication and machine-friendly precision.

**Sources:**

- Schakkal *et al.*, *“Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation,”* 2025 – VLM-based 3-tier control, using natural language goals and visual inputs for high-level planning[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=match%20at%20L360%20Formally%2C%20a,the%20conditions%20described%20by)[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=trained%20via%20imitation%20learning%20that,step%20manipulation). Demonstrated 72.5% success in multi-step tasks with VLM planner[arxiv.org](https://arxiv.org/html/2506.22827v1#:~:text=trained%20via%20imitation%20learning%20that,step%20manipulation).
- He *et al.* (BAIR), *“Goal Representations for Instruction Following (GRIF),”* 2023 – Combined language instructions and image goals for policy learning; visual goals improve grounding[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=Conditioning%20on%20visual%20goals%20%28i,pixel%20with%20other%20states) while language makes task-specification easier for humans[robohub.org](https://robohub.org/goal-representations-for-instruction-following/#:~:text=is%20to%20provide%20a%20goal,digesting%20large%20unstructured%20robot%20datasets).
- Huang *et al.*, *“Visual Planning: Let’s Think Only with Images,”* 2023 – Introduced planning purely via image sequences. Showed that for spatial tasks, image-based reasoning outperformed text-based (40% higher success in navigation)[arxiv.org](https://arxiv.org/html/2505.11409v1#:~:text=We%20validate%20the%20feasibility%20of,in%20the%20visual%20planning%20paradigm), highlighting the power of visual representations for planning.
- Cui *et al.*, *“Can Foundation Models Perform Zero-Shot Task Specification for Robot Manipulation?,”* 2022 – Explored goal modalities beyond state vectors: internet images, sketches, language descriptions. Found foundation models enabled promising zero-shot interpretation of these human-friendly goals[arxiv.org](https://arxiv.org/abs/2204.11134#:~:text=low,humans%20to%20specify%20and%20use)[arxiv.org](https://arxiv.org/abs/2204.11134#:~:text=goal%20specification%20that%20are%20expected,world%20datasets).
- SayCan (Ahn *et al.*, 2022) – Pioneering LLM-guided robot planning. Used a PaLM language model to generate feasible sub-tasks and a learned value function to ground them[say-can.github.io](https://say-can.github.io/#:~:text=The%20main%20principle%20that%20we,to%20weight%20the%20skill%27s%20likelihood)[say-can.github.io](https://say-can.github.io/#:~:text=In%20summary%2C%20given%20a%20high,output%20step%20is%20to%20terminate). Achieved 84% high-level plan success on open-vocabulary instructions by combining LLM knowledge with robot affordances[say-can.github.io](https://say-can.github.io/#:~:text=Image)[say-can.github.io](https://say-can.github.io/#:~:text=The%20proposed%20approach%20achieves%20an,enhancing%20the%20underlying%20language%20model).
- **Position Paper:** Davidson & Srinivasa, *“Human-like RL is facilitated by structured and expressive goals,”* 2023 – Discusses how different goal representations (reward functions, goal images, language, programs) trade off between ease of grounding and expressivity[guydavidson.me](https://guydavidson.me/files/position_human_like_rl_structured_expressive_goals.pdf#:~:text=Figure%201,but%20are%20substantially%20harder%20to). Concludes that language/programs enable abstract, compositional goals but are harder to ground, whereas direct observations are easy to ground but limited in expressiveness[guydavidson.me](https://guydavidson.me/files/position_human_like_rl_structured_expressive_goals.pdf#:~:text=Figure%201,but%20are%20substantially%20harder%20to). This aligns with our recommendation to combine modalities for the best of both worlds.