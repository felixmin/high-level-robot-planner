---
notion-id: 28d20c92-0436-80a2-bdf5-f8effe51ccbb
---
## Organizational

- Are there and can I participate in lab weeklies, paper discussions, presentations etc ? Where can I find dates and such?

## Operational

- How to access the workstations? Is there a list of the machines somewhere?
- Where can I get LEGO CAD models / assembly instructions?
- [https://doku.lrz.de/lrz-ai-systems-11484278.html](https://doku.lrz.de/lrz-ai-systems-11484278.html)

## Related Work

- Many papers use PAMDP and RL for high-level planners
    - PAMDP: Parametrized-action Markov decision process

[[Related Work]] 

## My thoughts

- Decisions that make sense for me
    - We predict steps (actions or states) not plans
        - No assembly graph or something
        - Humans don’t do that either… with a manual we already have the plan and we just intuitively do one step after another
→ We might have a memory of our past actions to know where we are
    - We use a foundational model for manual and current observation understanding and selecting the super high-level next step and then either
        - Output parametrized actions (directly or through a lower-level model), or
        - Output states (I think always requires a lower-level model e.g. masking)

- If we cannot create our own training data we are limited
    - How many CAD models do we have?
    - The ability to only work with finished CAD models and deduce the assembly instructions from there would unlock a lot of training data 
        - We have many CAD models
        - CAD models could possible even be AI generated (easier than assembly instructions)

- Human in the loop would be cool
    - Otherwise it is not really realistic and deployable to the real world
    - For human in the loop we need a VLM / VLA at the high level
    - Here likely the model predicts actions → predicting states would be weird here? But why actually?

- Self-training / self-improving would be cool
    - Requires moving parts in sim

- Predicting states vs predicting high-level actions
→ Is there actually a difference??? A state is always the result of an action???
    - Predicting actions
        - Can go lower level and adjust better
        - Always needs training
            - RL
                - Train from scratch 
                - Fine-tuning of pretrained models
            - IL (SFT) to leverage pretrained models
    - Predicting states
        - Can use 

- RL vs IL (incl pretrained + SFT)


- Single model vs multiple models
    - Can we have a single model that outputs a description of our 

- Concrete approaches

## Next steps

- Get data
- Understand data
- Get access to workstation try loading CAD models into sim and (re)moving parts
- Further analyze model / IL vs RL / etc decision and training pipeline
- Find tradeoff between cool and doable
    - Start simple easy
    - Then try more complex approach




## Notes

We dont need to consider unfeasible placements as exactly that is where the assembly instruction comes in
