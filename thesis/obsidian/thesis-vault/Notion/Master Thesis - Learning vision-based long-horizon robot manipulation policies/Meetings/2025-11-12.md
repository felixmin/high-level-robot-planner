---
notion-id: 2a920c92-0436-800d-b26e-c24bd27e7171
base: "[[Meetings.base]]"
Tags: []
Created: 2025-11-12T10:06:00
---
## Progress

- Trained LAPA LAQ model on a mix of something2something and my own data
    - Finetuning their weights and training from scatch both work
- Found POV lego building videos on youtube
- Built a tool to download videos from youtube and create frames from them

## Thoughts

- Should i build on top of theirs?
    - I dont like their stucture… no hydra, hard to run sweeps / experiments
    - I will probably adjust to my own structure… fast with genai anyway
    - I should build it such that I still can use their pretrained weights

→ Switch now already?

## Next steps

- Improve understanding of their implementation
- Fix my implementation
- Check whether I get the same results with theirs (also using their weights)
- Find a benchmark for the action latents

- Get better understanding for other action representations like motion tracks
- Can we use existing instructions to improve our latent space in the LAQ or find alternate representations

## Questions

- Can we improve the architecture? VQ-VAE is older → Can we improve LAPA?
    - Maybe longer horizon → currently more than 5 frames gets very blurry
- Maybe add motion tracks or use other representations
    - Can we fuse them somehow?
    - Predict in keypoint space not in image space?
    - Maybe cotracker
    - Peek paper → also uses cotracker
    - [https://arxiv.org/pdf/2311.01977](https://arxiv.org/pdf/2311.01977) figure 2
        - Overspecified / how to … 


- CMU meeting?


Ad link to youtube videos to canva

Share video downloading tool with lab github

Github handle an oliver

Add images and insights here to the docs

Image + sentence at least